
@misc{adefokunChessboard2022pull,
  title = {Chess-Board Pul Request},
  author = {Adefokun, Ahira},
  year = {2022},
  month = feb,
  url = {https://github.com/ahira-justice/chess-board/pull/5},
  urldate = {2022-04-10},
  abstract = {A chess board library for presenting game positions},
  copyright = {GPL-3.0}
}

@misc{adefokunChessboardAhirajustice2022,
  title = {Chess-Board Ahira-Justice},
  author = {Adefokun, Ahira},
  year = {2022},
  month = feb,
  url = {https://github.com/ahira-justice/chess-board},
  urldate = {2022-04-10},
  abstract = {A chess board library for presenting game positions},
  copyright = {GPL-3.0},
  keywords = {board,chess,display,pygame}
}

@article{AlphaBetaPruning2022,
  title = {Alpha\textendash Beta Pruning},
  year = {2022},
  month = jan,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Alpha%E2%80%93beta_pruning&oldid=1068746141},
  urldate = {2022-02-01},
  abstract = {Alpha\textendash beta pruning is a search algorithm that seeks to decrease the number of nodes that are evaluated by the minimax algorithm in its search tree. It is an adversarial search algorithm used commonly for machine playing of two-player games (Tic-tac-toe, Chess, Connect 4, etc.). It stops evaluating a move when at least one possibility has been found that proves the move to be worse than a previously examined move. Such moves need not be evaluated further. When applied to a standard minimax tree, it returns the same move as minimax would, but prunes away branches that cannot possibly influence the final decision.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1068746141},
  file = {/home/zjeffer/Documents/Zotero/storage/HY5HATSA/Alpha–beta_pruning.html}
}

@misc{AlphaGo,
  title = {{{AlphaGo}}},
  url = {https://www.deepmind.com/research/highlighted-research/alphago},
  urldate = {2022-04-07},
  abstract = {AlphaGo is the first computer program to defeat a professional human Go player, a landmark achievement that experts believe was a decade ahead of its time.},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/SE7K9AKV/alphago.html}
}

@article{AlphaGo2022a,
  title = {{{AlphaGo}}},
  year = {2022},
  month = mar,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=AlphaGo&oldid=1077595428},
  urldate = {2022-04-07},
  abstract = {AlphaGo is a computer program that plays the board game Go. It was developed by DeepMind Technologies a subsidiary of Google (now Alphabet Inc.). Subsequent versions of AlphaGo became increasingly powerful, including a version that competed under the name Master. After retiring from competitive play, AlphaGo Master was succeeded by an even more powerful version known as AlphaGo Zero, which was completely self-taught without learning from human games. AlphaGo Zero was then generalized into a program known as AlphaZero, which played additional games, including chess and shogi.  AlphaZero has in turn been succeeded by a program known as MuZero which learns without being taught the rules. AlphaGo and its successors use a Monte Carlo tree search algorithm to find its moves based on knowledge previously acquired by machine learning, specifically by an artificial neural network (a deep learning method) by extensive training, both from human and computer play. A neural network is trained to identify the best moves and the winning percentages of these moves. This neural network improves the strength of the tree search, resulting in stronger move selection in the next iteration. In October 2015, in a match against Fan Hui, the original AlphaGo became the first computer Go program to beat a human professional Go player without handicap on a full-sized 19\texttimes 19 board. In March 2016, it beat Lee Sedol in a five-game match, the first time a computer Go program has beaten a 9-dan professional without handicap. Although it lost to Lee Sedol in the fourth game, Lee resigned in the final game, giving a final score of 4 games to 1 in favour of AlphaGo. In recognition of the victory, AlphaGo was awarded an honorary 9-dan by the Korea Baduk Association. The lead up and the challenge match with Lee Sedol were documented in a documentary film also titled AlphaGo, directed by Greg Kohs. The win by AlphaGo was chosen by Science as one of the Breakthrough of the Year runners-up on 22 December 2016.At the 2017 Future of Go Summit, the Master version of AlphaGo beat Ke Jie, the number one ranked player in the world at the time, in a three-game match, after which AlphaGo was awarded professional 9-dan by the Chinese Weiqi Association.After the match between AlphaGo and Ke Jie, DeepMind retired AlphaGo, while continuing AI research in other areas. The self-taught AlphaGo Zero achieved a 100\textendash 0 victory against the early competitive version of AlphaGo, and its successor AlphaZero is currently perceived as the world's top player in Go.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1077595428},
  file = {/home/zjeffer/Documents/Zotero/storage/NL32PWCK/AlphaGo.html}
}

@article{AlphaGoZero2022,
  title = {{{AlphaGo Zero}}},
  year = {2022},
  month = feb,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=AlphaGo_Zero&oldid=1073216893},
  urldate = {2022-04-08},
  abstract = {AlphaGo Zero is a version of DeepMind's Go software AlphaGo. AlphaGo's team published an article in the journal Nature on 19 October 2017, introducing AlphaGo Zero, a version created without using data from human games, and stronger than any previous version. By playing games against itself, AlphaGo Zero surpassed the strength of AlphaGo Lee in three days by winning 100 games to 0, reached the level of AlphaGo Master in 21 days, and exceeded all the old versions in 40 days.Training artificial intelligence (AI) without datasets derived from human experts has significant implications for the development of AI with superhuman skills because expert data is "often expensive, unreliable or simply unavailable." Demis Hassabis, the co-founder and CEO of DeepMind, said that AlphaGo Zero was so powerful because it was "no longer constrained by the limits of human knowledge". Furthermore, AlphaGo Zero performed better than standard reinforcement deep learning models (such as DQN implementations) due to its integration of Monte Carlo tree search. David Silver, one of the first authors of DeepMind's papers published in Nature on AlphaGo, said that it is possible to have generalised AI algorithms by removing the need to learn from humans.Google later developed AlphaZero, a generalized version of AlphaGo Zero that could play chess and Sh\=ogi in addition to Go. In December 2017, AlphaZero beat the 3-day version of AlphaGo Zero by winning 60 games to 40, and with 8 hours of training it outperformed AlphaGo Lee on an Elo scale. AlphaZero also defeated a top chess program (Stockfish) and a top Sh\=ogi program (Elmo).},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1073216893},
  file = {/home/zjeffer/Documents/Zotero/storage/A439TYI3/AlphaGo_Zero.html}
}

@article{AlphaZero2022,
  title = {{{AlphaZero}}},
  year = {2022},
  month = jan,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=AlphaZero&oldid=1065791194},
  urldate = {2022-02-01},
  abstract = {AlphaZero is a computer program developed by artificial intelligence research company DeepMind to master the games of chess, shogi and go.  This algorithm uses an approach similar to AlphaGo Zero.  On December 5, 2017, the DeepMind team released a preprint introducing AlphaZero, which within 24 hours of training achieved a superhuman level of play in these three games by defeating world-champion programs Stockfish, elmo, and the three-day version of AlphaGo Zero. In each case it made use of custom tensor processing units (TPUs) that the Google programs were optimized to use. AlphaZero was trained solely via "self-play" using 5,000 first-generation TPUs to generate the games and 64 second-generation TPUs to train the neural networks, all in parallel, with no access to opening books or endgame tables. After four hours of training, DeepMind estimated AlphaZero was playing chess at a higher Elo rating than Stockfish 8; after nine hours of training, the algorithm defeated Stockfish 8 in a time-controlled 100-game tournament (28 wins, 0 losses, and 72 draws). The trained algorithm played on a single machine with four TPUs.  DeepMind's paper on AlphaZero was published in the journal Science on 7 December 2018. In 2019 DeepMind published a new paper detailing MuZero, a new algorithm able to generalise on AlphaZero work, playing both Atari and board games without knowledge of the rules or representations of the game.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1065791194}
}

@misc{AlphaZeroChessprogrammingWiki,
  title = {{{AlphaZero}} - {{Chessprogramming}} Wiki},
  url = {https://www.chessprogramming.org/AlphaZero},
  urldate = {2022-04-07},
  file = {/home/zjeffer/Documents/Zotero/storage/Y7EXS3FL/AlphaZero.html}
}

@misc{AlphaZeroSheddingNew,
  title = {{{AlphaZero}}: {{Shedding}} New Light on Chess, Shogi, and {{Go}}},
  shorttitle = {{{AlphaZero}}},
  url = {https://www.deepmind.com/blog/alphazero-shedding-new-light-on-chess-shogi-and-go},
  urldate = {2022-04-13},
  abstract = {In late 2017 we introduced AlphaZero, a single system that taught itself from scratch how to master the games of chess, shogi (Japanese chess), and Go, beating a world-champion program in each case. We were excited by the preliminary results and thrilled to see the response from members of the chess community, who saw in AlphaZero's games a ground-breaking, highly dynamic and ``unconventional'' style of play that differed from any chess playing engine that came before it.},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/QXSL36CH/alphazero-shedding-new-light-on-chess-shogi-and-go.html}
}

@article{ApplicationspecificIntegratedCircuit2022,
  title = {Application-Specific Integrated Circuit},
  year = {2022},
  month = feb,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Application-specific_integrated_circuit&oldid=1074023806},
  urldate = {2022-04-07},
  abstract = {An application-specific integrated circuit (ASIC ) is an integrated circuit (IC) chip customized for a particular use, rather than intended for general-purpose use. For example, a chip designed to run in a digital voice recorder or a high-efficiency video codec (e.g. AMD VCE) is an ASIC. Application-specific standard product (ASSP) chips are intermediate between ASICs and industry standard integrated circuits like the 7400 series or the 4000 series. ASIC chips are typically fabricated using metal-oxide-semiconductor (MOS) technology, as MOS integrated circuit chips.As feature sizes have shrunk and design tools improved over the years, the maximum complexity (and hence functionality) possible in an ASIC has grown from 5,000 logic gates to over 100 million. Modern ASICs often include entire microprocessors, memory blocks including ROM, RAM, EEPROM, flash memory and other large building blocks. Such an ASIC is often termed a SoC (system-on-chip). Designers of digital ASICs often use a hardware description language (HDL), such as Verilog or VHDL, to describe the functionality of ASICs.Field-programmable gate arrays (FPGA) are the modern-day technology for building a breadboard or prototype from standard parts; programmable logic blocks and programmable interconnects allow the same FPGA to be used in many different applications. For smaller designs or lower production volumes, FPGAs may be more cost-effective than an ASIC design, even in production. The non-recurring engineering (NRE) cost of an ASIC can run into the millions of dollars. Therefore, device manufacturers typically prefer FPGAs for prototyping and devices with low production volume and ASICs for very large production volumes where NRE costs can be amortized across many devices.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1074023806},
  file = {/home/zjeffer/Documents/Zotero/storage/8FB2J84R/Application-specific_integrated_circuit.html}
}

@misc{blogArtworkPersonalizationNetflix2017,
  title = {Artwork {{Personalization}} at {{Netflix}}},
  author = {Blog, Netflix Technology},
  year = {2017},
  month = dec,
  journal = {Medium},
  url = {https://netflixtechblog.com/artwork-personalization-c589f074ad76},
  urldate = {2022-04-13},
  abstract = {Artwork is the first instance of personalizing not just what we recommend but also how we recommend.},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/2IN2WS4Z/artwork-personalization-c589f074ad76.html}
}

@misc{bodensteinAlphaZero2019,
  title = {{{AlphaZero}} |},
  author = {Bodenstein, Sebastian},
  year = {2019},
  month = sep,
  url = {https://sebastianbodenstein.net/post/alphazero/},
  urldate = {2022-04-09},
  abstract = {AlphaZero is a landmark result in Artificial Intelligence research: it is a single algorithm that mastered Chess, Go and Shogi having access to only the game rules. And `mastered' here means beating the worlds strongest chess engines (an open source implementation of AlphaZero, Leela Zero, is now the official computer chess world champion ), and easily beating the version of AlphaGo that beat Lee Sedol. It has also achieved some fame in the wider world: it might not have its own documentary like AlphaGo , but it does have its own New Yorker profile !},
  langid = {american},
  file = {/home/zjeffer/Documents/Zotero/storage/I2FEBK58/alphazero.html}
}

@misc{BranchingFactorChessprogramming,
  title = {Branching {{Factor}} - {{Chessprogramming}} Wiki},
  url = {https://www.chessprogramming.org/Branching_Factor},
  urldate = {2022-03-28},
  file = {/home/zjeffer/Documents/Zotero/storage/6PJVRU9Q/Branching_Factor.html}
}

@article{ChessEngine2022,
  title = {Chess Engine},
  year = {2022},
  month = apr,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Chess_engine&oldid=1080874516},
  urldate = {2022-04-05},
  abstract = {In computer chess, a chess engine is a computer program that analyzes chess or chess variant positions, and generates a move or list of moves that it regards as strongest.A chess engine is usually a back end with a command-line interface with no graphics or windowing.  Engines are usually used with a front end, a windowed graphical user interface such as Chessbase or WinBoard that the user can interact with via a keyboard, mouse or touchscreen.  This allows the user to play against multiple engines without learning a new user interface for each, and allows different engines to play against each other. Many chess engines are now available for mobile phones and tablets, making them even more accessible.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1080874516},
  file = {/home/zjeffer/Documents/Zotero/storage/ZNMWQYKQ/Chess_engine.html}
}

@misc{CloudComputingServices,
  title = {Cloud {{Computing Services}} ~|~ {{Google Cloud}}},
  url = {https://cloud.google.com/},
  urldate = {2022-04-13},
  file = {/home/zjeffer/Documents/Zotero/storage/8AK3LNNQ/cloud.google.com.html}
}

@misc{decemberHasGoogleCracked2020,
  title = {Has {{Google}} Cracked the Data Center Cooling Problem with {{AI}}?},
  author = {December, Soumik Roy | 27 and {2018}},
  year = {2020},
  month = may,
  journal = {Tech Wire Asia},
  url = {https://techwireasia.com/2020/05/has-google-cracked-the-data-centre-cooling-problem-with-ai/},
  urldate = {2022-04-13},
  abstract = {Data center cooling costs are swallowing money from cloud and tech giants, but Google may have found a solution with AI.},
  langid = {american},
  file = {/home/zjeffer/Documents/Zotero/storage/D75JCDT2/has-google-cracked-the-data-centre-cooling-problem-with-ai.html}
}

@misc{DeepMind,
  title = {{{DeepMind}} | {{About}}},
  url = {https://www.deepmind.com/about},
  urldate = {2022-04-13},
  abstract = {DeepMind is a cutting-edge Artificial Intelligence company of scientists, engineers, and researchers dedicated to using technology for solving the world's greatest problems.},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/FZN6AHGG/about.html}
}

@article{DeepMind2022,
  title = {{{DeepMind}}},
  year = {2022},
  month = feb,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=DeepMind&oldid=1072182749},
  urldate = {2022-04-07},
  abstract = {DeepMind Technologies is a British artificial intelligence subsidiary of Alphabet Inc. and research laboratory founded in September 2010. DeepMind was acquired by Google in 2014. The company is based in London, with research centres in Canada, France, and the United States. In 2015, it became a wholly owned subsidiary of Alphabet Inc, Google's parent company. DeepMind has created a neural network that learns how to play video games in a fashion similar to that of humans, as well as a Neural Turing machine, or a neural network that may be able to access an external memory like a conventional Turing machine, resulting in a computer that mimics the short-term memory of the human brain. DeepMind made headlines in 2016 after its AlphaGo program beat a human professional Go player Lee Sedol, the world champion, in a five-game match, which was the subject of a documentary film. A more general program, AlphaZero, beat the most powerful programs playing go, chess and shogi (Japanese chess) after a few days of play against itself using reinforcement learning. In 2020, DeepMind made significant advances in the problem of protein folding.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1072182749},
  file = {/home/zjeffer/Documents/Zotero/storage/YSTYP9BP/DeepMind.html}
}

@misc{deepmindAlphaGoMovieFull2020,
  title = {{{AlphaGo}} - {{The Movie}} | {{Full}} Award-Winning Documentary},
  author = {{DeepMind}},
  year = {2020},
  month = mar,
  url = {https://www.youtube.com/watch?v=WXuK6gekU1Y},
  urldate = {2022-04-07},
  abstract = {With more board configurations than there are atoms in the universe, the ancient Chinese game of Go has long been considered a grand challenge for artificial intelligence.  On March 9, 2016, the worlds of Go and artificial intelligence collided in South Korea for an extraordinary best-of-five-game competition, coined The DeepMind Challenge Match. Hundreds of millions of people around the world watched as a legendary Go master took on an unproven AI challenger for the first time in history. Directed by Greg Kohs and with an original score by Academy Award nominee Hauschka, AlphaGo had its premiere at the Tribeca Film Festival. It has since gone on to win countless awards and near universal praise for a story that chronicles a journey from the halls of Oxford, through the backstreets of Bordeaux, past the coding terminals of DeepMind in London, and ultimately, to the seven-day tournament in Seoul. As the drama unfolds, more questions emerge: What can artificial intelligence reveal about a 3000-year-old game? What can it teach us about humanity? Best documentary winner: Denver International Film Festival (2017), Warsaw International Film Festival (2017), and Traverse City Film Festival (2017). Official selection at Tribeca Film Festival (2017), BFI London Film Festival (2017), and Critics' Choice Documentary Awards (2017). Find out more: https://www.alphagomovie.com/ -- "I want my style of Go to be something different, something new, my own thing, something that no one has thought of before." Lee Sedol, Go Champion (18 World Titles). "We think of DeepMind as kind of an Apollo program effort for AI. Our mission is to fundamentally understand intelligence and recreate it artificially." Demis Hassabis, Co-Founder \& CEO, DeepMind. "The Game of Go is the holy grail of artificial intelligence. Everything we've ever tried in AI, it just falls over when you try the game of Go." Dave Silver, Lead Researcher for AlphaGo.}
}

@misc{dukezhouAnswerAlphaZeroExample2018,
  title = {Answer to "{{Is AlphaZero}} an Example of an {{AGI}}?"},
  shorttitle = {Answer to "{{Is AlphaZero}} an Example of an {{AGI}}?},
  author = {DukeZhou},
  year = {2018},
  month = nov,
  journal = {Artificial Intelligence Stack Exchange},
  url = {https://ai.stackexchange.com/a/9170/54037},
  urldate = {2022-04-13},
  file = {/home/zjeffer/Documents/Zotero/storage/FVSVEJGE/is-alphazero-an-example-of-an-agi.html}
}

@misc{eppesHowComputerizedChess2019,
  title = {How a {{Computerized Chess Opponent}} ``{{Thinks}}'' \textemdash{} {{The Minimax Algorithm}}},
  author = {Eppes, Marissa},
  year = {2019},
  month = oct,
  journal = {Medium},
  url = {https://towardsdatascience.com/how-a-chess-playing-computer-thinks-about-its-next-move-8f028bd0e7b1},
  urldate = {2022-04-05},
  abstract = {In 1997, a computer named ``Deep Blue'' defeated reigning world chess champion Garry Kasparov{$\mkern1mu$}\textemdash{$\mkern1mu$}a defining moment in the history AI theory.},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/6HVW2HDS/how-a-chess-playing-computer-thinks-about-its-next-move-8f028bd0e7b1.html}
}

@article{EvaluationFunction2022,
  title = {Evaluation Function},
  year = {2022},
  month = mar,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Evaluation_function&oldid=1079533564},
  urldate = {2022-04-06},
  abstract = {An evaluation function, also known as a heuristic evaluation function or static evaluation function, is a function used by game-playing computer programs to estimate the value or goodness of a position (usually at a leaf or terminal node) in a game tree. Most of the time, the value is either a real number or a quantized integer, often in nths of the value of a playing piece such as a stone in go or a pawn in chess, where n may be tenths, hundredths or other convenient fraction, but sometimes, the value is an array of three values in the unit interval, representing the win, draw, and loss percentages of the position.  There do not exist analytical or theoretical models for evaluation functions for unsolved games, nor are such functions entirely ad-hoc.  The composition of evaluation functions is determined empirically by inserting a candidate function into an automaton and evaluating its subsequent performance.  A significant body of evidence now exists for several games like chess, shogi and go as to the general composition of evaluation functions for them. Games in which game playing computer programs employ evaluation functions include chess, go, shogi (Japanese chess), othello, hex, backgammon, and checkers. In addition, with the advent of programs such as MuZero, computer programs also use evaluation functions to play video games, such as those from the Atari 2600. Some games like tic-tac-toe are strongly solved, and do not require search or evaluation because a discrete solution tree is available.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1079533564},
  file = {/home/zjeffer/Documents/Zotero/storage/H6UN66AT/Evaluation_function.html}
}

@article{FirstmoveAdvantageChess2022,
  title = {First-Move Advantage in Chess},
  year = {2022},
  month = mar,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=First-move_advantage_in_chess&oldid=1080096428},
  urldate = {2022-04-10},
  abstract = {In chess, there is a general consensus among players and theorists that the player who makes the first move (White) has an inherent advantage. Since 1851, compiled statistics support this view; White consistently wins slightly more often than Black, usually scoring between 52 and 56 percent. White's winning percentage is about the same for tournament games between humans and games between computers; however, White's advantage is less significant in blitz games and games between novices. Chess players and theoreticians have long debated whether, given perfect play by both sides, the game should end in a win for White or a draw. Since approximately 1889, when World Champion Wilhelm Steinitz addressed this issue, the consensus has been that a perfectly played game would end in a draw (futile game). A few notable players have argued, however, that White's advantage may be sufficient to force a win: Weaver Adams and Vsevolod Rauzer claimed that White is winning after the first move 1.e4, while Hans Berliner argued that 1.d4 may win for White. Chess is not a solved game, however, and it is considered unlikely that the game will be solved in the foreseeable future. Some players, including world champions such as Jos\'e Ra\'ul Capablanca, Emanuel Lasker, and Bobby Fischer, have expressed fears of a "draw death" as chess becomes more deeply analyzed. To alleviate this danger, Capablanca and Fischer both proposed chess variants to revitalize the game, while Lasker suggested changing how draws and stalemate are scored. Some writers have challenged the view that White has an inherent advantage. Grandmaster (GM) Andr\'as Adorj\'an wrote a series of books on the theme that "Black is OK!", arguing that the general perception that White has an advantage is founded more in psychology than reality. GM Mihai Suba and others contend that sometimes White's initiative disappears for no apparent reason as a game progresses. The prevalent style of play for Black today is to seek unbalanced, dynamic positions with active counterplay, rather than merely trying to equalize. Modern writers also argue that Black has certain countervailing advantages. The consensus that White should try to win can be a psychological burden for the white player, who sometimes loses by trying too hard to win. Some symmetrical openings (i.e. those where Black's moves mirror White's) can lead to situations where moving first is a detriment, for either psychological or objective reasons.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1080096428},
  file = {/home/zjeffer/Documents/Zotero/storage/ZEI36RPV/First-move_advantage_in_chess.html}
}

@article{ForsythEdwardsNotation2022,
  title = {Forsyth\textendash{{Edwards Notation}}},
  year = {2022},
  month = feb,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Forsyth%E2%80%93Edwards_Notation&oldid=1069540048},
  urldate = {2022-04-09},
  abstract = {Forsyth\textendash Edwards Notation (FEN) is a standard notation for describing a particular board position of a chess game. The purpose of FEN is to provide all the necessary information to restart a game from a particular position. FEN is based on a system developed by Scottish newspaper journalist David Forsyth. Forsyth's system became popular in the 19th century; Steven J. Edwards extended it to support use by computers. FEN is defined in the "Portable Game Notation Specification and Implementation Guide". In the  Portable Game Notation for chess games, FEN is used to define initial positions other than the standard one. FEN does not provide sufficient information to decide whether a draw by threefold repetition may be legally claimed or a draw offer may be accepted; for that, a different format such as Extended Position Description is needed.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1069540048},
  file = {/home/zjeffer/Documents/Zotero/storage/YMRPTXHV/Forsyth–Edwards_Notation.html}
}

@article{GoGame2022,
  title = {Go (Game)},
  year = {2022},
  month = mar,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Go_(game)&oldid=1079941654},
  urldate = {2022-04-06},
  abstract = {Go or Weiqi, Weichi (simplified Chinese: 围棋; traditional Chinese: 圍棋; pinyin: w\'eiq\'i) is an abstract strategy board game for two players in which the aim is to surround more territory than the opponent. The game was invented in China more than 2,500 years ago and is believed to be the oldest board game continuously played to the present day. A 2016 survey by the International Go Federation's 75 member nations found that there are over 46 million people worldwide who know how to play Go and over 20 million current players, the majority of whom live in East Asia.The playing pieces are called stones. One player uses the white stones and the other, black. The players take turns placing the stones on the vacant intersections (points) of a board. Once placed on the board, stones may not be moved, but stones are removed from the board if the stone (or group of stones) is surrounded by opposing stones on all orthogonally adjacent points, in which case the stone or group is captured. The game proceeds until neither player wishes to make another move. When a game concludes, the winner is determined by counting each player's surrounded territory along with captured stones and komi (points added to the score of the player with the white stones as compensation for playing second). Games may also be terminated by resignation. The standard Go board has a 19\texttimes 19 grid of lines, containing 361 points. Beginners often play on smaller 9\texttimes 9 and 13\texttimes 13 boards, and archaeological evidence shows that the game was played in earlier centuries on a board with a 17\texttimes 17 grid. However, boards with a 19\texttimes 19 grid had become standard by the time the game reached Korea in the 5th century CE and Japan in the 7th century CE.Go was considered one of the four essential arts of the cultured aristocratic Chinese scholars in antiquity. The earliest written reference to the game is generally recognized as the historical annal Zuo Zhuan (c. 4th century BCE).Despite its relatively simple rules, Go is extremely complex. Compared to chess, Go has both a larger board with more scope for play and longer games and, on average, many more alternatives to consider per move. The number of legal board positions in Go has been calculated to be approximately 2.1\texttimes 10170, which is vastly greater than the number of atoms in the observable universe, estimated to be of the order of 1080.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1079941654},
  file = {/home/zjeffer/Documents/Zotero/storage/CRB5BTLU/Go_(game).html}
}

@misc{Graphviz,
  title = {Graphviz},
  journal = {Graphviz},
  url = {https://graphviz.org/},
  urldate = {2022-04-09},
  abstract = {Graph Visualization Software},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/QD5LSY5Q/graphviz.org.html}
}

@misc{Lc02022,
  title = {Lc0},
  year = {2022},
  month = apr,
  url = {https://github.com/LeelaChessZero/lc0},
  urldate = {2022-04-08},
  abstract = {The rewritten engine, originally for tensorflow. Now all other backends have been ported here.},
  copyright = {GPL-3.0},
  howpublished = {LCZero}
}

@article{LeelaChessZero2022,
  title = {Leela {{Chess Zero}}},
  year = {2022},
  month = apr,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Leela_Chess_Zero&oldid=1080962634},
  urldate = {2022-04-08},
  abstract = {Leela Chess Zero (abbreviated as LCZero, lc0) is a free, open-source, and deep neural network\textendash based chess engine and distributed computing project. Development has been spearheaded by programmer Gary Linscott, who is also a developer for the Stockfish chess engine. Leela Chess Zero was adapted from the Leela Zero Go engine, which in turn was based on Google's AlphaGo Zero project. One of the purposes of Leela Chess Zero was to verify the methods in the AlphaZero paper as applied to the game of chess. Like Leela Zero and AlphaGo Zero, Leela Chess Zero starts with no intrinsic chess-specific knowledge other than the basic rules of the game. Leela Chess Zero then learns how to play chess by reinforcement learning from repeated self-play, using a distributed computing network coordinated at the Leela Chess Zero website. As of January 2022, Leela Chess Zero has played over 500 million games against itself, playing around 1 million games every day, and is capable of play at a level that is comparable with Stockfish, the leading conventional chess program.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1080962634},
  file = {/home/zjeffer/Documents/Zotero/storage/HELIZIGW/Leela_Chess_Zero.html}
}

@article{LeelaZero2021,
  title = {Leela {{Zero}}},
  year = {2021},
  month = oct,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Leela_Zero&oldid=1049955001},
  urldate = {2022-04-08},
  abstract = {Leela Zero is a free and open-source computer Go program released on 25 October 2017. It is developed by Belgian programmer Gian-Carlo Pascutto, the author of chess engine Sjeng and Go engine Leela.Leela Zero's algorithm is based on DeepMind's 2017 paper about AlphaGo Zero. Unlike the original Leela, which has a lot of human knowledge and heuristics programmed into it, the program code in Leela Zero only knows the basic rules and nothing more.  The knowledge that makes Leela Zero a strong player is contained in a neural network, which is trained based on the results of previous games that the program played.Leela Zero is trained by a distributed effort, which is coordinated at the Leela Zero website. Members of the community provide computing resources by running the client, which generates self-play games and submits them to the server. The self-play games are used to train newer networks. Generally, over 500 clients have connected to the server to contribute resources. The community has provided high quality code contributions as well.Leela Zero finished third at the BerryGenomics Cup World AI Go Tournament in Fuzhou, Fujian, China on 28 April 2018. The New Yorker at the end of 2018 characterized Leela and Leela Zero as "the world's most successful open-source Go engines".In early 2018, another team branched Leela Chess Zero from the same code base, also to verify the methods in the AlphaZero paper as applied to the game of chess. AlphaZero's use of Google TPUs was replaced by a crowd-sourcing infrastructure and the ability to use graphics card GPUs via the OpenCL library. Even so, it is expected to take a year of crowd-sourced training to make up for the dozen hours that AlphaZero was allowed to train for its chess match in the paper.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1049955001},
  file = {/home/zjeffer/Documents/Zotero/storage/IWYWG9ZC/Leela_Zero.html}
}

@misc{LichessOrgOpen,
  title = {Lichess.Org Open Database},
  url = {https://database.lichess.org/},
  urldate = {2022-04-09},
  file = {/home/zjeffer/Documents/Zotero/storage/ZKWLE8MJ/database.lichess.org.html}
}

@misc{MasteringGameGo,
  title = {Mastering the Game of {{Go}} with {{Deep Neural Networks}} \& {{Tree Search}}},
  url = {https://www.deepmind.com/publications/mastering-the-game-of-go-with-deep-neural-networks-tree-search},
  urldate = {2022-04-07},
  abstract = {A new approach to computer Go that combines Monte-Carlo tree search with  deep neural networks that have been trained by supervised learning, from  human expert games, and by reinforcement learning, from games of self-play.  The first time ever that a computer program has defeated a human  professional player.},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/8E6TT26Q/mastering-the-game-of-go-with-deep-neural-networks-tree-search.html}
}

@misc{MasteringGameZero,
  title = {Mastering the Game of {{Go}} without {{Human Knowledge}}},
  url = {https://www.deepmind.com/publications/mastering-the-game-of-go-without-human-knowledge},
  urldate = {2022-04-07},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that  learns, *tabula rasa*, superhuman proficiency in challenging domains.  Recently, AlphaGo became the first program to defeat a world champion in  the game of Go. The tree search in AlphaGo evaluated positions and selected  moves using deep neural networks. These neural networks were trained by  supervised learning from human expert moves, and by reinforcement learning  from selfplay. Here, we introduce an algorithm based solely on  reinforcement learning, without human data, guidance, or domain knowledge  beyond game rules. AlphaGo becomes its own teacher: a neural network is  trained to predict AlphaGo's own move selections and also the winner of  AlphaGo's games. This neural network improves the strength of tree search,  resulting in higher quality move selection and stronger self-play in the  next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved  superhuman performance, winning 100-0 against the previously published,  champion-defeating AlphaGo.},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/JQCFTJDT/mastering-the-game-of-go-without-human-knowledge.html}
}

@article{Minimax2022,
  title = {Minimax},
  year = {2022},
  month = mar,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Minimax&oldid=1076761456},
  urldate = {2022-04-05},
  abstract = {Minimax (sometimes MinMax, MM or saddle point) is a decision rule used in artificial intelligence, decision theory, game theory, statistics, and philosophy for minimizing the possible loss for a worst case (maximum loss) scenario.  When dealing with gains, it is referred to as "maximin"\textemdash to maximize the minimum gain.  Originally formulated for n-player zero-sum game theory, covering both the cases where players take alternate moves and those where they make simultaneous moves, it has also been extended to more complex games and to general decision-making in the presence of uncertainty.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1076761456},
  file = {/home/zjeffer/Documents/Zotero/storage/RV43RPZZ/Minimax.html}
}

@misc{MinimaxMonteCarlo,
  title = {Minimax and {{Monte Carlo Tree Search}} - {{Philipp Muens}}},
  url = {https://philippmuens.com/minimax-and-mcts},
  urldate = {2022-04-06},
  abstract = {Understanding the underpinnings of modern Game AIs.},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/I9WAA6KK/minimax-and-mcts.html}
}

@misc{MLMonteCarlo2019,
  title = {{{ML}} | {{Monte Carlo Tree Search}} ({{MCTS}})},
  year = {2019},
  month = jan,
  journal = {GeeksforGeeks},
  url = {https://www.geeksforgeeks.org/ml-monte-carlo-tree-search-mcts/},
  urldate = {2022-04-07},
  abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
  chapter = {Technical Scripter},
  langid = {american},
  file = {/home/zjeffer/Documents/Zotero/storage/XIRFETNI/ml-monte-carlo-tree-search-mcts.html}
}

@article{MonteCarloTree2022,
  title = {Monte {{Carlo}} Tree Search},
  year = {2022},
  month = apr,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Monte_Carlo_tree_search&oldid=1081107255},
  urldate = {2022-04-06},
  abstract = {In computer science, Monte Carlo tree search (MCTS) is a heuristic search algorithm for some kinds of decision processes, most notably those employed in software that plays board games. In that context MCTS is used to solve the game tree.  MCTS was combined with neural networks in 2016 for computer Go. It has been used in other board games like chess and shogi, games with incomplete information such as bridge and poker, as well as in turn-based-strategy video games (such as Total War: Rome II's implementation in the high level campaign AI). MCTS has also been used in self-driving cars, for example in Tesla's Autopilot software.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1081107255},
  file = {/home/zjeffer/Documents/Zotero/storage/UXBP4BLN/Monte_Carlo_tree_search.html}
}

@misc{nbroAnswerAreThere2020,
  title = {Answer to "{{Are}} There Any Applications of Reinforcement Learning Other than Games?"},
  shorttitle = {Answer to "{{Are}} There Any Applications of Reinforcement Learning Other than Games?},
  author = {{nbro}},
  year = {2020},
  month = nov,
  journal = {Artificial Intelligence Stack Exchange},
  url = {https://ai.stackexchange.com/a/24355/54037},
  urldate = {2022-04-13}
}

@misc{NeuralNetworksChessprogramming,
  title = {Neural {{Networks}} - {{Chessprogramming}} Wiki},
  url = {https://www.chessprogramming.org/Neural_Networks#ANNs},
  urldate = {2022-04-07},
  file = {/home/zjeffer/Documents/Zotero/storage/6PJ47EN6/Neural_Networks.html}
}

@misc{pygame,
  title = {{{PyGame}}},
  journal = {PyGame},
  url = {https://www.pygame.org/news},
  urldate = {2022-04-10},
  file = {/home/zjeffer/Documents/Zotero/storage/CFWE8IKH/news.html}
}

@misc{PythonchessChessLibrarya,
  title = {Python-Chess: A Chess Library for {{Python}} \textemdash{} Python-Chess 1.9.0 Documentation},
  url = {https://python-chess.readthedocs.io/en/latest/},
  urldate = {2022-04-08},
  file = {/home/zjeffer/Documents/Zotero/storage/76UIPQ5X/latest.html}
}

@article{silverMasteringChessShogi2017a,
  title = {Mastering {{Chess}} and {{Shogi}} by {{Self-Play}} with a {{General Reinforcement Learning Algorithm}}},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.01815 [cs]},
  eprint = {1712.01815},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1712.01815},
  urldate = {2022-02-01},
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/zjeffer/Documents/Zotero/storage/ZRRMASQ3/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf;/home/zjeffer/Documents/Zotero/storage/CIJ93UYW/1712.html}
}

@misc{slaterAnswerAreThere2017,
  title = {Answer to "{{Are}} There Any Applications of Reinforcement Learning Other than Games?"},
  shorttitle = {Answer to "{{Are}} There Any Applications of Reinforcement Learning Other than Games?},
  author = {Slater, Neil},
  year = {2017},
  month = jul,
  journal = {Artificial Intelligence Stack Exchange},
  url = {https://ai.stackexchange.com/a/3730/54037},
  urldate = {2022-04-13},
  file = {/home/zjeffer/Documents/Zotero/storage/SRCK4F9F/are-there-any-applications-of-reinforcement-learning-other-than-games.html}
}

@article{StockfishChess2022,
  title = {Stockfish (Chess)},
  year = {2022},
  month = mar,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Stockfish_(chess)&oldid=1079146589},
  urldate = {2022-03-28},
  abstract = {Stockfish is a free and open-source chess engine, available for various desktop and mobile platforms. It can be used in chess software through the Universal Chess Interface. Stockfish is consistently ranked first or near the top of most chess-engine rating lists and is the strongest CPU chess engine in the world. It has won the Top Chess Engine Championship 11 times. Stockfish is developed by Marco Costalba, Joona Kiiski, Gary Linscott, Tord Romstad, St\'ephane Nicolet, Stefan Geschwentner, and Joost VandeVondele, with many contributions from a community of open-source developers. It is derived from Glaurung, an open-source engine by Tord Romstad released in 2004.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1079146589},
  file = {/home/zjeffer/Documents/Zotero/storage/FPIS5XAZ/Stockfish_(chess).html}
}

@misc{surmenokContextualBanditsReinforcement2017,
  title = {Contextual {{Bandits}} and {{Reinforcement Learning}}},
  author = {Surmenok, Pavel},
  year = {2017},
  month = oct,
  journal = {Medium},
  url = {https://towardsdatascience.com/contextual-bandits-and-reinforcement-learning-6bdfeaece72a},
  urldate = {2022-04-13},
  abstract = {If you develop personalization of user experience for your website or an app, contextual bandits can help you. Using contextual bandits\ldots},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/W7M4MUFB/contextual-bandits-and-reinforcement-learning-6bdfeaece72a.html}
}

@article{TensorProcessingUnit2022,
  title = {Tensor {{Processing Unit}}},
  year = {2022},
  month = mar,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Tensor_Processing_Unit&oldid=1077688658},
  urldate = {2022-04-07},
  abstract = {Tensor Processing Unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google specifically for neural network machine learning, particularly using Google's own TensorFlow software. Google began using TPUs internally in 2015, and in 2018 made them available for third party use, both as part of its cloud infrastructure and by offering a smaller version of the chip for sale.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1077688658},
  file = {/home/zjeffer/Documents/Zotero/storage/HUNP7D9R/Tensor_Processing_Unit.html}
}

@misc{ThomasMoerlandPostdoc,
  title = {{Thomas Moerland \textendash{} Postdoc, Leiden University, The Netherlands}},
  url = {https://thomasmoerland.nl/},
  urldate = {2022-04-13},
  langid = {dutch},
  file = {/home/zjeffer/Documents/Zotero/storage/G56YLFZN/thomasmoerland.nl.html}
}

@article{TopChessEngine2022,
  title = {Top {{Chess Engine Championship}}},
  year = {2022},
  month = mar,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Top_Chess_Engine_Championship&oldid=1077985240},
  urldate = {2022-04-12},
  abstract = {Top Chess Engine Championship, formerly known as Thoresen Chess Engines Competition  (TCEC or nTCEC), is a computer chess tournament that has been run since 2010. It was organized, directed, and hosted by Martin Thoresen until the end of Season 6; from Season 7 onward it has been organized by Chessdom. It is often regarded as the Unofficial World Computer Chess Championship because of its strong participant line-up and long time-control matches on high-end hardware, giving rise to very high-class chess. The tournament has attracted nearly all the top engines compared to the World Computer Chess Championship. After a short break in 2012, TCEC was restarted in early 2013 (as nTCEC) and is currently active (renamed as TCEC in early 2014) with 24/7 live broadcasts of chess matches on its website.  Since season 5, TCEC has been sponsored by Chessdom Arena.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1077985240},
  file = {/home/zjeffer/Documents/Zotero/storage/2AMB254S/Top_Chess_Engine_Championship.html}
}

@misc{zjefferChessboardZjeffer2022,
  title = {Chess-Board Zjeffer},
  author = {{zjeffer}},
  year = {2022},
  month = jan,
  url = {https://github.com/zjeffer/chess-board},
  urldate = {2022-04-10},
  abstract = {A chess board library for presenting game positions},
  copyright = {GPL-3.0}
}

@misc{zjefferChessdeeprlcpp2022,
  title = {Chess-Deep-Rl-Cpp},
  author = {{zjeffer}},
  year = {2022},
  month = apr,
  url = {https://github.com/zjeffer/chess-deep-rl-cpp},
  urldate = {2022-04-10},
  abstract = {C++ version of my chess-deep-rl project. WIP},
  keywords = {ai,alphazero,artificial-intelligence,chess,chess-engine,deep-learning,deep-reinforcement-learning,machine-learning,mcts,neural-networks,reinforcement-learning}
}

@misc{zjefferChessEngineDeep2022,
  title = {Chess Engine with {{Deep Reinforcement}} Learning},
  author = {{zjeffer}},
  year = {2022},
  month = feb,
  url = {https://github.com/zjeffer/chess-deep-rl},
  urldate = {2022-04-09},
  abstract = {Research project: create a chess engine using Deep Reinforcement Learning},
  copyright = {GPL-3.0},
  keywords = {ai,alphazero,artificial-intelligence,chess,chess-engine,deep-learning,deep-reinforcement-learning,machine-learning,mcts,neural-network,neural-networks,reinforcement-learning}
}


