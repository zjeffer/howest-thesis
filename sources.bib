
@misc{adefokunChessboard2022pull,
  title = {Chess-Board Pul Request},
  author = {Adefokun, Ahira},
  year = {2022},
  month = feb,
  url = {https://github.com/ahira-justice/chess-board/pull/5},
  urldate = {2022-04-10},
  abstract = {A chess board library for presenting game positions},
  copyright = {GPL-3.0}
}

@misc{adefokunChessboardAhirajustice2022,
  title = {Chess-Board Ahira-Justice},
  author = {Adefokun, Ahira},
  year = {2022},
  month = feb,
  url = {https://github.com/ahira-justice/chess-board},
  urldate = {2022-04-10},
  abstract = {A chess board library for presenting game positions},
  copyright = {GPL-3.0},
  keywords = {board,chess,display,pygame}
}

@article{AlphaBetaPruning2022,
  title = {Alpha\textendash Beta Pruning},
  year = {2022},
  month = jan,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Alpha%E2%80%93beta_pruning&oldid=1068746141},
  urldate = {2022-02-01},
  abstract = {Alpha\textendash beta pruning is a search algorithm that seeks to decrease the number of nodes that are evaluated by the minimax algorithm in its search tree. It is an adversarial search algorithm used commonly for machine playing of two-player games (Tic-tac-toe, Chess, Connect 4, etc.). It stops evaluating a move when at least one possibility has been found that proves the move to be worse than a previously examined move. Such moves need not be evaluated further. When applied to a standard minimax tree, it returns the same move as minimax would, but prunes away branches that cannot possibly influence the final decision.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1068746141},
  file = {/home/zjeffer/Documents/Zotero/storage/HY5HATSA/Alpha–beta_pruning.html}
}

@misc{AlphaGo,
  title = {{{AlphaGo}}},
  url = {https://www.deepmind.com/research/highlighted-research/alphago},
  urldate = {2022-04-07},
  abstract = {AlphaGo is the first computer program to defeat a professional human Go player, a landmark achievement that experts believe was a decade ahead of its time.},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/SE7K9AKV/alphago.html}
}

@article{AlphaGo2022a,
  title = {{{AlphaGo}}},
  year = {2022},
  month = mar,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=AlphaGo&oldid=1077595428},
  urldate = {2022-04-07},
  abstract = {AlphaGo is a computer program that plays the board game Go. It was developed by DeepMind Technologies a subsidiary of Google (now Alphabet Inc.). Subsequent versions of AlphaGo became increasingly powerful, including a version that competed under the name Master. After retiring from competitive play, AlphaGo Master was succeeded by an even more powerful version known as AlphaGo Zero, which was completely self-taught without learning from human games. AlphaGo Zero was then generalized into a program known as AlphaZero, which played additional games, including chess and shogi.  AlphaZero has in turn been succeeded by a program known as MuZero which learns without being taught the rules. AlphaGo and its successors use a Monte Carlo tree search algorithm to find its moves based on knowledge previously acquired by machine learning, specifically by an artificial neural network (a deep learning method) by extensive training, both from human and computer play. A neural network is trained to identify the best moves and the winning percentages of these moves. This neural network improves the strength of the tree search, resulting in stronger move selection in the next iteration. In October 2015, in a match against Fan Hui, the original AlphaGo became the first computer Go program to beat a human professional Go player without handicap on a full-sized 19\texttimes 19 board. In March 2016, it beat Lee Sedol in a five-game match, the first time a computer Go program has beaten a 9-dan professional without handicap. Although it lost to Lee Sedol in the fourth game, Lee resigned in the final game, giving a final score of 4 games to 1 in favour of AlphaGo. In recognition of the victory, AlphaGo was awarded an honorary 9-dan by the Korea Baduk Association. The lead up and the challenge match with Lee Sedol were documented in a documentary film also titled AlphaGo, directed by Greg Kohs. The win by AlphaGo was chosen by Science as one of the Breakthrough of the Year runners-up on 22 December 2016.At the 2017 Future of Go Summit, the Master version of AlphaGo beat Ke Jie, the number one ranked player in the world at the time, in a three-game match, after which AlphaGo was awarded professional 9-dan by the Chinese Weiqi Association.After the match between AlphaGo and Ke Jie, DeepMind retired AlphaGo, while continuing AI research in other areas. The self-taught AlphaGo Zero achieved a 100\textendash 0 victory against the early competitive version of AlphaGo, and its successor AlphaZero is currently perceived as the world's top player in Go.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1077595428},
  file = {/home/zjeffer/Documents/Zotero/storage/NL32PWCK/AlphaGo.html}
}

@article{AlphaGoZero2022,
  title = {{{AlphaGo Zero}}},
  year = {2022},
  month = feb,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=AlphaGo_Zero&oldid=1073216893},
  urldate = {2022-04-08},
  abstract = {AlphaGo Zero is a version of DeepMind's Go software AlphaGo. AlphaGo's team published an article in the journal Nature on 19 October 2017, introducing AlphaGo Zero, a version created without using data from human games, and stronger than any previous version. By playing games against itself, AlphaGo Zero surpassed the strength of AlphaGo Lee in three days by winning 100 games to 0, reached the level of AlphaGo Master in 21 days, and exceeded all the old versions in 40 days.Training artificial intelligence (AI) without datasets derived from human experts has significant implications for the development of AI with superhuman skills because expert data is "often expensive, unreliable or simply unavailable." Demis Hassabis, the co-founder and CEO of DeepMind, said that AlphaGo Zero was so powerful because it was "no longer constrained by the limits of human knowledge". Furthermore, AlphaGo Zero performed better than standard reinforcement deep learning models (such as DQN implementations) due to its integration of Monte Carlo tree search. David Silver, one of the first authors of DeepMind's papers published in Nature on AlphaGo, said that it is possible to have generalised AI algorithms by removing the need to learn from humans.Google later developed AlphaZero, a generalized version of AlphaGo Zero that could play chess and Sh\=ogi in addition to Go. In December 2017, AlphaZero beat the 3-day version of AlphaGo Zero by winning 60 games to 40, and with 8 hours of training it outperformed AlphaGo Lee on an Elo scale. AlphaZero also defeated a top chess program (Stockfish) and a top Sh\=ogi program (Elmo).},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1073216893},
  file = {/home/zjeffer/Documents/Zotero/storage/A439TYI3/AlphaGo_Zero.html}
}

@article{AlphaZero2022,
  title = {{{AlphaZero}}},
  year = {2022},
  month = jan,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=AlphaZero&oldid=1065791194},
  urldate = {2022-02-01},
  abstract = {AlphaZero is a computer program developed by artificial intelligence research company DeepMind to master the games of chess, shogi and go.  This algorithm uses an approach similar to AlphaGo Zero.  On December 5, 2017, the DeepMind team released a preprint introducing AlphaZero, which within 24 hours of training achieved a superhuman level of play in these three games by defeating world-champion programs Stockfish, elmo, and the three-day version of AlphaGo Zero. In each case it made use of custom tensor processing units (TPUs) that the Google programs were optimized to use. AlphaZero was trained solely via "self-play" using 5,000 first-generation TPUs to generate the games and 64 second-generation TPUs to train the neural networks, all in parallel, with no access to opening books or endgame tables. After four hours of training, DeepMind estimated AlphaZero was playing chess at a higher Elo rating than Stockfish 8; after nine hours of training, the algorithm defeated Stockfish 8 in a time-controlled 100-game tournament (28 wins, 0 losses, and 72 draws). The trained algorithm played on a single machine with four TPUs.  DeepMind's paper on AlphaZero was published in the journal Science on 7 December 2018. In 2019 DeepMind published a new paper detailing MuZero, a new algorithm able to generalise on AlphaZero work, playing both Atari and board games without knowledge of the rules or representations of the game.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1065791194}
}

@misc{AlphaZeroChessprogrammingWiki,
  title = {{{AlphaZero}} - {{Chessprogramming}} Wiki},
  url = {https://www.chessprogramming.org/AlphaZero},
  urldate = {2022-04-07},
  file = {/home/zjeffer/Documents/Zotero/storage/Y7EXS3FL/AlphaZero.html}
}

@misc{AlphaZeroSheddingNew,
  title = {{{AlphaZero}}: {{Shedding}} New Light on Chess, Shogi, and {{Go}}},
  shorttitle = {{{AlphaZero}}},
  url = {https://www.deepmind.com/blog/alphazero-shedding-new-light-on-chess-shogi-and-go},
  urldate = {2022-04-13},
  abstract = {In late 2017 we introduced AlphaZero, a single system that taught itself from scratch how to master the games of chess, shogi (Japanese chess), and Go, beating a world-champion program in each case. We were excited by the preliminary results and thrilled to see the response from members of the chess community, who saw in AlphaZero's games a ground-breaking, highly dynamic and ``unconventional'' style of play that differed from any chess playing engine that came before it.},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/QXSL36CH/alphazero-shedding-new-light-on-chess-shogi-and-go.html}
}

@article{ApplicationspecificIntegratedCircuit2022,
  title = {Application-Specific Integrated Circuit},
  year = {2022},
  month = feb,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Application-specific_integrated_circuit&oldid=1074023806},
  urldate = {2022-04-07},
  abstract = {An application-specific integrated circuit (ASIC ) is an integrated circuit (IC) chip customized for a particular use, rather than intended for general-purpose use. For example, a chip designed to run in a digital voice recorder or a high-efficiency video codec (e.g. AMD VCE) is an ASIC. Application-specific standard product (ASSP) chips are intermediate between ASICs and industry standard integrated circuits like the 7400 series or the 4000 series. ASIC chips are typically fabricated using metal-oxide-semiconductor (MOS) technology, as MOS integrated circuit chips.As feature sizes have shrunk and design tools improved over the years, the maximum complexity (and hence functionality) possible in an ASIC has grown from 5,000 logic gates to over 100 million. Modern ASICs often include entire microprocessors, memory blocks including ROM, RAM, EEPROM, flash memory and other large building blocks. Such an ASIC is often termed a SoC (system-on-chip). Designers of digital ASICs often use a hardware description language (HDL), such as Verilog or VHDL, to describe the functionality of ASICs.Field-programmable gate arrays (FPGA) are the modern-day technology for building a breadboard or prototype from standard parts; programmable logic blocks and programmable interconnects allow the same FPGA to be used in many different applications. For smaller designs or lower production volumes, FPGAs may be more cost-effective than an ASIC design, even in production. The non-recurring engineering (NRE) cost of an ASIC can run into the millions of dollars. Therefore, device manufacturers typically prefer FPGAs for prototyping and devices with low production volume and ASICs for very large production volumes where NRE costs can be amortized across many devices.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1074023806},
  file = {/home/zjeffer/Documents/Zotero/storage/8FB2J84R/Application-specific_integrated_circuit.html}
}

@misc{BachelorThesisYour,
  title = {Bachelor Thesis: Your Opinion on {{AlphaZero}}'s Results - {{TalkChess}}.Com},
  url = {https://talkchess.com/forum3/viewtopic.php?f=2&t=79694},
  urldate = {2022-05-28},
  file = {/home/zjeffer/Documents/Zotero/storage/UDJ3A7YG/viewtopic.html}
}

@misc{blogArtworkPersonalizationNetflix2017,
  title = {Artwork {{Personalization}} at {{Netflix}}},
  author = {Blog, Netflix Technology},
  year = {2017},
  month = dec,
  journal = {Medium},
  url = {https://netflixtechblog.com/artwork-personalization-c589f074ad76},
  urldate = {2022-04-13},
  abstract = {Artwork is the first instance of personalizing not just what we recommend but also how we recommend.},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/2IN2WS4Z/artwork-personalization-c589f074ad76.html}
}

@misc{blogSinglePlayerAlphaZeroa,
  title = {A {{Single-Player Alpha Zero Implementation}} in 250 {{Lines}} of {{Python}}},
  author = {Blog, Thomas Moerland{\textasciiacute}s},
  url = {https://tmoer.github.io/AlphaZero/},
  urldate = {2022-05-28},
  abstract = {Alpha Zero has recently changed the state-of-the-art of Artificial Intelligence (AI) performance in the game of Go, Chess and Shogi. In this blog post, I have implemented the AlphaZero algorithm for single player games. There are a few small modifications on my side to make it suitable for this setting, but these are rather small and explicitly mentioned in the text below. The core functionality (except some generic helper functions) takes merely \textasciitilde 250 lines of annoted Python code (including blank lines), contained in a single script. The main point of this blog post is to illustrate the potential simplicity of an AlphaZero implementation, and provide a baseline for interested people to experiment with.},
  file = {/home/zjeffer/Documents/Zotero/storage/D32VJ8SQ/AlphaZero.html}
}

@misc{bodensteinAlphaZero2019,
  title = {{{AlphaZero}} |},
  author = {Bodenstein, Sebastian},
  year = {2019},
  month = sep,
  url = {https://sebastianbodenstein.net/post/alphazero/},
  urldate = {2022-04-09},
  abstract = {AlphaZero is a landmark result in Artificial Intelligence research: it is a single algorithm that mastered Chess, Go and Shogi having access to only the game rules. And `mastered' here means beating the worlds strongest chess engines (an open source implementation of AlphaZero, Leela Zero, is now the official computer chess world champion ), and easily beating the version of AlphaGo that beat Lee Sedol. It has also achieved some fame in the wider world: it might not have its own documentary like AlphaGo , but it does have its own New Yorker profile !},
  langid = {american},
  file = {/home/zjeffer/Documents/Zotero/storage/I2FEBK58/alphazero.html}
}

@misc{BranchingFactorChessprogramming,
  title = {Branching {{Factor}} - {{Chessprogramming}} Wiki},
  url = {https://www.chessprogramming.org/Branching_Factor},
  urldate = {2022-03-28},
  file = {/home/zjeffer/Documents/Zotero/storage/6PJVRU9Q/Branching_Factor.html}
}

@article{Castling2022,
  title = {Castling},
  year = {2022},
  month = may,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Castling&oldid=1088552051},
  urldate = {2022-05-28},
  abstract = {Castling is a move in the game of chess in which a player moves the king two squares toward a rook on the same rank and moves the rook to the square that the king has crossed. It is the only move in chess in which a player moves two pieces in the same move, and it is the only two-square king move.Castling may be done only if neither the king nor the rook has previously moved, the squares between the king and the rook are unoccupied, the king is not in check, and the king does not cross over or end up on a square attacked by an opposing piece. Castling with the king's rook is known as castling kingside or castling short, and castling with the queen's rook is known as castling queenside or castling long; here, short and long refer to the distance the rook moves. The notation for castling, in both the algebraic and descriptive systems, is 0-0 for castling kingside and 0-0-0 for castling queenside. Castling originates from the king's leap, a two-square king move added to European chess between the 14th and 15th centuries, and took on its present form in the 17th century; however, local variations in castling rules were common, persisting in Italy until the late 19th century. Castling does not exist in Asian games of the chess family, such as shogi, xiangqi, and janggi, but it is commonly included in variants of Western chess.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1088552051},
  file = {/home/zjeffer/Documents/Zotero/storage/IRZ6DWN7/Castling.html}
}

@article{Chess2022a,
  title = {Chess},
  year = {2022},
  month = may,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Chess&oldid=1085844722},
  urldate = {2022-05-07},
  abstract = {Chess is a board game played between two players. It is sometimes called Western chess or international chess to distinguish it from related games such as xiangqi and shogi. The current form of the game emerged in Southern Europe during the second half of the 15th century after evolving from chaturanga, a similar but much older game of Indian origin. Today, chess is one of the world's most popular games, played by millions of people worldwide. Chess is an abstract strategy game and involves no hidden information. It is played on a square chessboard with 64 squares arranged in an eight-by-eight grid. At the start, each player (one controlling the white pieces, the other controlling the black pieces) controls sixteen pieces: one king, one queen, two rooks, two bishops, two knights, and eight pawns. The object of the game is to checkmate the opponent's king, whereby the king is under immediate attack (in "check") and there is no way for it to escape. There are also several ways a game can end in a draw. Organized chess arose in the 19th century. Chess competition today is governed internationally by FIDE (International Chess Federation). The first universally recognized World Chess Champion, Wilhelm Steinitz, claimed his title in 1886; Magnus Carlsen is the current World Champion. A huge body of chess theory has developed since the game's inception. Aspects of art are found in chess composition, and chess in its turn influenced Western culture and art and has connections with other fields such as mathematics, computer science, and psychology. One of the goals of early computer scientists was to create a chess-playing machine. In 1997, Deep Blue became the first computer to beat the reigning World Champion in a match when it defeated Garry Kasparov. Today's chess engines are significantly stronger than the best human players, and have deeply influenced the development of chess theory.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1085844722},
  file = {/home/zjeffer/Documents/Zotero/storage/X86FEYJ4/Chess.html}
}

@misc{ChessBoardEditor,
  title = {Chess Board Editor},
  journal = {lichess.org},
  url = {https://lichess.org/editor},
  urldate = {2022-05-07},
  abstract = {Load opening positions or create your own chess position on a chess board editor},
  langid = {american},
  file = {/home/zjeffer/Documents/Zotero/storage/L3XIH9CI/editor.html}
}

@article{ChessEngine2022,
  title = {Chess Engine},
  year = {2022},
  month = apr,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Chess_engine&oldid=1080874516},
  urldate = {2022-04-05},
  abstract = {In computer chess, a chess engine is a computer program that analyzes chess or chess variant positions, and generates a move or list of moves that it regards as strongest.A chess engine is usually a back end with a command-line interface with no graphics or windowing.  Engines are usually used with a front end, a windowed graphical user interface such as Chessbase or WinBoard that the user can interact with via a keyboard, mouse or touchscreen.  This allows the user to play against multiple engines without learning a new user interface for each, and allows different engines to play against each other. Many chess engines are now available for mobile phones and tablets, making them even more accessible.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1080874516},
  file = {/home/zjeffer/Documents/Zotero/storage/ZNMWQYKQ/Chess_engine.html}
}

@misc{CloudComputingServices,
  title = {Cloud {{Computing Services}} ~|~ {{Google Cloud}}},
  url = {https://cloud.google.com/},
  urldate = {2022-04-13},
  file = {/home/zjeffer/Documents/Zotero/storage/8AK3LNNQ/cloud.google.com.html}
}

@misc{decemberHasGoogleCracked2020,
  title = {Has {{Google}} Cracked the Data Center Cooling Problem with {{AI}}?},
  author = {December, Soumik Roy | 27 and {2018}},
  year = {2020},
  month = may,
  journal = {Tech Wire Asia},
  url = {https://techwireasia.com/2020/05/has-google-cracked-the-data-centre-cooling-problem-with-ai/},
  urldate = {2022-04-13},
  abstract = {Data center cooling costs are swallowing money from cloud and tech giants, but Google may have found a solution with AI.},
  langid = {american},
  file = {/home/zjeffer/Documents/Zotero/storage/D75JCDT2/has-google-cracked-the-data-centre-cooling-problem-with-ai.html}
}

@misc{DeepMind,
  title = {{{DeepMind}} | {{About}}},
  url = {https://www.deepmind.com/about},
  urldate = {2022-04-13},
  abstract = {DeepMind is a cutting-edge Artificial Intelligence company of scientists, engineers, and researchers dedicated to using technology for solving the world's greatest problems.},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/FZN6AHGG/about.html}
}

@article{DeepMind2022,
  title = {{{DeepMind}}},
  year = {2022},
  month = feb,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=DeepMind&oldid=1072182749},
  urldate = {2022-04-07},
  abstract = {DeepMind Technologies is a British artificial intelligence subsidiary of Alphabet Inc. and research laboratory founded in September 2010. DeepMind was acquired by Google in 2014. The company is based in London, with research centres in Canada, France, and the United States. In 2015, it became a wholly owned subsidiary of Alphabet Inc, Google's parent company. DeepMind has created a neural network that learns how to play video games in a fashion similar to that of humans, as well as a Neural Turing machine, or a neural network that may be able to access an external memory like a conventional Turing machine, resulting in a computer that mimics the short-term memory of the human brain. DeepMind made headlines in 2016 after its AlphaGo program beat a human professional Go player Lee Sedol, the world champion, in a five-game match, which was the subject of a documentary film. A more general program, AlphaZero, beat the most powerful programs playing go, chess and shogi (Japanese chess) after a few days of play against itself using reinforcement learning. In 2020, DeepMind made significant advances in the problem of protein folding.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1072182749},
  file = {/home/zjeffer/Documents/Zotero/storage/YSTYP9BP/DeepMind.html}
}

@misc{deepmindAlphaGoMovieFull2020,
  title = {{{AlphaGo}} - {{The Movie}} | {{Full}} Award-Winning Documentary},
  author = {{DeepMind}},
  year = {2020},
  month = mar,
  url = {https://www.youtube.com/watch?v=WXuK6gekU1Y},
  urldate = {2022-04-07},
  abstract = {With more board configurations than there are atoms in the universe, the ancient Chinese game of Go has long been considered a grand challenge for artificial intelligence.  On March 9, 2016, the worlds of Go and artificial intelligence collided in South Korea for an extraordinary best-of-five-game competition, coined The DeepMind Challenge Match. Hundreds of millions of people around the world watched as a legendary Go master took on an unproven AI challenger for the first time in history. Directed by Greg Kohs and with an original score by Academy Award nominee Hauschka, AlphaGo had its premiere at the Tribeca Film Festival. It has since gone on to win countless awards and near universal praise for a story that chronicles a journey from the halls of Oxford, through the backstreets of Bordeaux, past the coding terminals of DeepMind in London, and ultimately, to the seven-day tournament in Seoul. As the drama unfolds, more questions emerge: What can artificial intelligence reveal about a 3000-year-old game? What can it teach us about humanity? Best documentary winner: Denver International Film Festival (2017), Warsaw International Film Festival (2017), and Traverse City Film Festival (2017). Official selection at Tribeca Film Festival (2017), BFI London Film Festival (2017), and Critics' Choice Documentary Awards (2017). Find out more: https://www.alphagomovie.com/ -- "I want my style of Go to be something different, something new, my own thing, something that no one has thought of before." Lee Sedol, Go Champion (18 World Titles). "We think of DeepMind as kind of an Apollo program effort for AI. Our mission is to fundamentally understand intelligence and recreate it artificially." Demis Hassabis, Co-Founder \& CEO, DeepMind. "The Game of Go is the holy grail of artificial intelligence. Everything we've ever tried in AI, it just falls over when you try the game of Go." Dave Silver, Lead Researcher for AlphaGo.}
}

@article{DeepReinforcementLearning2022a,
  title = {Deep Reinforcement Learning},
  year = {2022},
  month = mar,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Deep_reinforcement_learning&oldid=1078792888},
  urldate = {2022-04-14},
  abstract = {Deep reinforcement learning (deep RL) is a subfield of machine learning that combines reinforcement learning (RL) and deep learning. RL considers the problem of a computational agent learning to make decisions by trial and error. Deep RL incorporates deep learning into the solution, allowing agents to make decisions from unstructured input data without manual engineering of the state space. Deep RL algorithms are able to take in very large inputs (e.g. every pixel rendered to the screen in a video game) and decide what actions to perform to optimize an objective (eg. maximizing the game score). Deep reinforcement learning has been used for a diverse set of applications including but not limited to robotics, video games, natural language processing, computer vision, education, transportation, finance and healthcare.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1078792888},
  file = {/home/zjeffer/Documents/Zotero/storage/W5IQ74XS/Deep_reinforcement_learning.html}
}

@misc{DirichletDistributionWikipedia,
  title = {Dirichlet Distribution - {{Wikipedia}}},
  url = {https://en.wikipedia.org/wiki/Dirichlet_distribution},
  urldate = {2022-05-21},
  file = {/home/zjeffer/Documents/Zotero/storage/RD2ZU2I6/Dirichlet_distribution.html}
}

@misc{discordLC0Discord,
  title = {{{LC0 Discord}}},
  author = {Discord},
  url = {https://discord.gg/pKujYxD}
}

@misc{dukezhouAnswerAlphaZeroExample2018,
  title = {Answer to "{{Is AlphaZero}} an Example of an {{AGI}}?"},
  shorttitle = {Answer to "{{Is AlphaZero}} an Example of an {{AGI}}?},
  author = {DukeZhou},
  year = {2018},
  month = nov,
  journal = {Artificial Intelligence Stack Exchange},
  url = {https://ai.stackexchange.com/a/9170/54037},
  urldate = {2022-04-13},
  file = {/home/zjeffer/Documents/Zotero/storage/FVSVEJGE/is-alphazero-an-example-of-an-agi.html}
}

@article{EloRatingSystem2022,
  title = {Elo Rating System},
  year = {2022},
  month = may,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Elo_rating_system&oldid=1088992853},
  urldate = {2022-05-28},
  abstract = {The Elo rating system is a method for calculating the relative skill levels of players in zero-sum games such as chess. It is named after its creator Arpad Elo, a Hungarian-American physics professor. The Elo system was originally invented as an improved chess-rating system over the previously used Harkness system, but is also used as a rating system in association football, American football, baseball, basketball, pool, table tennis, Go, board games such as Scrabble and Diplomacy, and esports. The difference in the ratings between two players serves as a predictor of the outcome of a match. Two players with equal ratings who play against each other are expected to score an equal number of wins. A player whose rating is 100 points greater than their opponent's is expected to score 64\%; if the difference is 200 points, then the expected score for the stronger player is 76\%. A player's Elo rating is represented by a number which may change depending on the outcome of rated games played. After every game, the winning player takes points from the losing one. The difference between the ratings of the winner and loser determines the total number of points gained or lost after a game. If the higher-rated player wins, then only a few rating points will be taken from the lower-rated player. However, if the lower-rated player scores an upset win, many rating points will be transferred. The lower-rated player will also gain a few points from the higher rated player in the event of a draw. This means that this rating system is self-correcting. Players whose ratings are too low or too high should, in the long run, do better or worse correspondingly than the rating system predicts and thus gain or lose rating points until the ratings reflect their true playing strength. Elo ratings are a comparative only, and are valid only within the rating pool in which they were calculated, rather than being an absolute measure of a player's strength.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1088992853},
  file = {/home/zjeffer/Documents/Zotero/storage/Z7ULSDEU/Elo_rating_system.html}
}

@misc{eppesHowComputerizedChess2019,
  title = {How a {{Computerized Chess Opponent}} ``{{Thinks}}'' \textemdash{} {{The Minimax Algorithm}}},
  author = {Eppes, Marissa},
  year = {2019},
  month = oct,
  journal = {Medium},
  url = {https://towardsdatascience.com/how-a-chess-playing-computer-thinks-about-its-next-move-8f028bd0e7b1},
  urldate = {2022-04-05},
  abstract = {In 1997, a computer named ``Deep Blue'' defeated reigning world chess champion Garry Kasparov{$\mkern1mu$}\textemdash{$\mkern1mu$}a defining moment in the history AI theory.},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/6HVW2HDS/how-a-chess-playing-computer-thinks-about-its-next-move-8f028bd0e7b1.html}
}

@article{EvaluationFunction2022,
  title = {Evaluation Function},
  year = {2022},
  month = mar,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Evaluation_function&oldid=1079533564},
  urldate = {2022-04-06},
  abstract = {An evaluation function, also known as a heuristic evaluation function or static evaluation function, is a function used by game-playing computer programs to estimate the value or goodness of a position (usually at a leaf or terminal node) in a game tree. Most of the time, the value is either a real number or a quantized integer, often in nths of the value of a playing piece such as a stone in go or a pawn in chess, where n may be tenths, hundredths or other convenient fraction, but sometimes, the value is an array of three values in the unit interval, representing the win, draw, and loss percentages of the position.  There do not exist analytical or theoretical models for evaluation functions for unsolved games, nor are such functions entirely ad-hoc.  The composition of evaluation functions is determined empirically by inserting a candidate function into an automaton and evaluating its subsequent performance.  A significant body of evidence now exists for several games like chess, shogi and go as to the general composition of evaluation functions for them. Games in which game playing computer programs employ evaluation functions include chess, go, shogi (Japanese chess), othello, hex, backgammon, and checkers. In addition, with the advent of programs such as MuZero, computer programs also use evaluation functions to play video games, such as those from the Atari 2600. Some games like tic-tac-toe are strongly solved, and do not require search or evaluation because a discrete solution tree is available.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1079533564},
  file = {/home/zjeffer/Documents/Zotero/storage/H6UN66AT/Evaluation_function.html}
}

@article{FirstmoveAdvantageChess2022,
  title = {First-Move Advantage in Chess},
  year = {2022},
  month = mar,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=First-move_advantage_in_chess&oldid=1080096428},
  urldate = {2022-04-10},
  abstract = {In chess, there is a general consensus among players and theorists that the player who makes the first move (White) has an inherent advantage. Since 1851, compiled statistics support this view; White consistently wins slightly more often than Black, usually scoring between 52 and 56 percent. White's winning percentage is about the same for tournament games between humans and games between computers; however, White's advantage is less significant in blitz games and games between novices. Chess players and theoreticians have long debated whether, given perfect play by both sides, the game should end in a win for White or a draw. Since approximately 1889, when World Champion Wilhelm Steinitz addressed this issue, the consensus has been that a perfectly played game would end in a draw (futile game). A few notable players have argued, however, that White's advantage may be sufficient to force a win: Weaver Adams and Vsevolod Rauzer claimed that White is winning after the first move 1.e4, while Hans Berliner argued that 1.d4 may win for White. Chess is not a solved game, however, and it is considered unlikely that the game will be solved in the foreseeable future. Some players, including world champions such as Jos\'e Ra\'ul Capablanca, Emanuel Lasker, and Bobby Fischer, have expressed fears of a "draw death" as chess becomes more deeply analyzed. To alleviate this danger, Capablanca and Fischer both proposed chess variants to revitalize the game, while Lasker suggested changing how draws and stalemate are scored. Some writers have challenged the view that White has an inherent advantage. Grandmaster (GM) Andr\'as Adorj\'an wrote a series of books on the theme that "Black is OK!", arguing that the general perception that White has an advantage is founded more in psychology than reality. GM Mihai Suba and others contend that sometimes White's initiative disappears for no apparent reason as a game progresses. The prevalent style of play for Black today is to seek unbalanced, dynamic positions with active counterplay, rather than merely trying to equalize. Modern writers also argue that Black has certain countervailing advantages. The consensus that White should try to win can be a psychological burden for the white player, who sometimes loses by trying too hard to win. Some symmetrical openings (i.e. those where Black's moves mirror White's) can lead to situations where moving first is a detriment, for either psychological or objective reasons.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1080096428},
  file = {/home/zjeffer/Documents/Zotero/storage/ZEI36RPV/First-move_advantage_in_chess.html}
}

@article{FischerRandomChess2022,
  title = {Fischer Random Chess},
  year = {2022},
  month = apr,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Fischer_random_chess&oldid=1081800892},
  urldate = {2022-05-07},
  abstract = {Fischer random chess, also known as Chess960, is a variation of the game of chess invented by the former World Chess Champion Bobby Fischer. Fischer announced this variation on June 19, 1996, in Buenos Aires, Argentina. Fischer random chess employs the same board and pieces as classical chess, but the starting position of the pieces on the players' home ranks is randomized, following certain rules. The random setup makes gaining an advantage through the memorization of openings impracticable; players instead must rely more on their skill and creativity over the board. Randomizing the main pieces had long been known as shuffle chess, but Fischer random chess introduces new rules for the initial random setup, "preserving the dynamic nature of the game by retaining bishops of opposite colours for each player and the right to castle for both sides". The result is 960 unique possible starting positions. In 2008, FIDE added Chess960 to an appendix of the Laws of Chess. The first world championship officially sanctioned by FIDE, the FIDE World Fischer Random Chess Championship 2019, brought additional prominence to the variant.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1081800892},
  file = {/home/zjeffer/Documents/Zotero/storage/2WNY6SKJ/Fischer_random_chess.html}
}

@article{ForsythEdwardsNotation2022,
  title = {Forsyth\textendash{{Edwards Notation}}},
  year = {2022},
  month = feb,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Forsyth%E2%80%93Edwards_Notation&oldid=1069540048},
  urldate = {2022-04-09},
  abstract = {Forsyth\textendash Edwards Notation (FEN) is a standard notation for describing a particular board position of a chess game. The purpose of FEN is to provide all the necessary information to restart a game from a particular position. FEN is based on a system developed by Scottish newspaper journalist David Forsyth. Forsyth's system became popular in the 19th century; Steven J. Edwards extended it to support use by computers. FEN is defined in the "Portable Game Notation Specification and Implementation Guide". In the  Portable Game Notation for chess games, FEN is used to define initial positions other than the standard one. FEN does not provide sufficient information to decide whether a draw by threefold repetition may be legally claimed or a draw offer may be accepted; for that, a different format such as Extended Position Description is needed.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1069540048},
  file = {/home/zjeffer/Documents/Zotero/storage/YMRPTXHV/Forsyth–Edwards_Notation.html}
}

@article{GoGame2022,
  title = {Go (Game)},
  year = {2022},
  month = mar,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Go_(game)&oldid=1079941654},
  urldate = {2022-04-06},
  abstract = {Go or Weiqi, Weichi (simplified Chinese: 围棋; traditional Chinese: 圍棋; pinyin: w\'eiq\'i) is an abstract strategy board game for two players in which the aim is to surround more territory than the opponent. The game was invented in China more than 2,500 years ago and is believed to be the oldest board game continuously played to the present day. A 2016 survey by the International Go Federation's 75 member nations found that there are over 46 million people worldwide who know how to play Go and over 20 million current players, the majority of whom live in East Asia.The playing pieces are called stones. One player uses the white stones and the other, black. The players take turns placing the stones on the vacant intersections (points) of a board. Once placed on the board, stones may not be moved, but stones are removed from the board if the stone (or group of stones) is surrounded by opposing stones on all orthogonally adjacent points, in which case the stone or group is captured. The game proceeds until neither player wishes to make another move. When a game concludes, the winner is determined by counting each player's surrounded territory along with captured stones and komi (points added to the score of the player with the white stones as compensation for playing second). Games may also be terminated by resignation. The standard Go board has a 19\texttimes 19 grid of lines, containing 361 points. Beginners often play on smaller 9\texttimes 9 and 13\texttimes 13 boards, and archaeological evidence shows that the game was played in earlier centuries on a board with a 17\texttimes 17 grid. However, boards with a 19\texttimes 19 grid had become standard by the time the game reached Korea in the 5th century CE and Japan in the 7th century CE.Go was considered one of the four essential arts of the cultured aristocratic Chinese scholars in antiquity. The earliest written reference to the game is generally recognized as the historical annal Zuo Zhuan (c. 4th century BCE).Despite its relatively simple rules, Go is extremely complex. Compared to chess, Go has both a larger board with more scope for play and longer games and, on average, many more alternatives to consider per move. The number of legal board positions in Go has been calculated to be approximately 2.1\texttimes 10170, which is vastly greater than the number of atoms in the observable universe, estimated to be of the order of 1080.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1079941654},
  file = {/home/zjeffer/Documents/Zotero/storage/CRB5BTLU/Go_(game).html}
}

@misc{Graphviz,
  title = {Graphviz},
  journal = {Graphviz},
  url = {https://graphviz.org/},
  urldate = {2022-04-09},
  abstract = {Graph Visualization Software},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/QD5LSY5Q/graphviz.org.html}
}

@article{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  journal = {arXiv:1512.03385 [cs]},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1512.03385},
  urldate = {2022-05-07},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/zjeffer/Documents/Zotero/storage/Z8BFB24H/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/home/zjeffer/Documents/Zotero/storage/6YKV6WQA/1512.html}
}

@misc{HowAIHelps2018,
  title = {How {{AI}} Helps Better Manage and Run Data Centers},
  year = {2018},
  month = dec,
  journal = {Tech Wire Asia},
  url = {https://techwireasia.com/2018/12/how-ai-helps-better-manage-and-run-data-centers/},
  urldate = {2022-04-14},
  abstract = {With just a little support from maintenance staff, infrastructure experts are coming to the conclusion that AI can actually run data centers better \textemdash{} delivering consistency, optimal performance, and cost reductions \textemdash{} all at once},
  langid = {american},
  file = {/home/zjeffer/Documents/Zotero/storage/7GM7D4WB/how-ai-helps-better-manage-and-run-data-centers.html}
}

@misc{IntroductionGRPC,
  title = {Introduction to {{gRPC}}},
  journal = {gRPC},
  url = {https://grpc.io/docs/what-is-grpc/introduction/},
  urldate = {2022-04-15},
  abstract = {An introduction to gRPC and protocol buffers.},
  chapter = {docs},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/6BDGP82E/introduction.html}
}

@misc{Lc02022,
  title = {Lc0},
  year = {2022},
  month = apr,
  url = {https://github.com/LeelaChessZero/lc0},
  urldate = {2022-04-08},
  abstract = {The rewritten engine, originally for tensorflow. Now all other backends have been ported here.},
  copyright = {GPL-3.0},
  howpublished = {LCZero}
}

@article{LeelaChessZero2022,
  title = {Leela {{Chess Zero}}},
  year = {2022},
  month = apr,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Leela_Chess_Zero&oldid=1080962634},
  urldate = {2022-04-08},
  abstract = {Leela Chess Zero (abbreviated as LCZero, lc0) is a free, open-source, and deep neural network\textendash based chess engine and distributed computing project. Development has been spearheaded by programmer Gary Linscott, who is also a developer for the Stockfish chess engine. Leela Chess Zero was adapted from the Leela Zero Go engine, which in turn was based on Google's AlphaGo Zero project. One of the purposes of Leela Chess Zero was to verify the methods in the AlphaZero paper as applied to the game of chess. Like Leela Zero and AlphaGo Zero, Leela Chess Zero starts with no intrinsic chess-specific knowledge other than the basic rules of the game. Leela Chess Zero then learns how to play chess by reinforcement learning from repeated self-play, using a distributed computing network coordinated at the Leela Chess Zero website. As of January 2022, Leela Chess Zero has played over 500 million games against itself, playing around 1 million games every day, and is capable of play at a level that is comparable with Stockfish, the leading conventional chess program.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1080962634},
  file = {/home/zjeffer/Documents/Zotero/storage/HELIZIGW/Leela_Chess_Zero.html}
}

@article{LeelaZero2021,
  title = {Leela {{Zero}}},
  year = {2021},
  month = oct,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Leela_Zero&oldid=1049955001},
  urldate = {2022-04-08},
  abstract = {Leela Zero is a free and open-source computer Go program released on 25 October 2017. It is developed by Belgian programmer Gian-Carlo Pascutto, the author of chess engine Sjeng and Go engine Leela.Leela Zero's algorithm is based on DeepMind's 2017 paper about AlphaGo Zero. Unlike the original Leela, which has a lot of human knowledge and heuristics programmed into it, the program code in Leela Zero only knows the basic rules and nothing more.  The knowledge that makes Leela Zero a strong player is contained in a neural network, which is trained based on the results of previous games that the program played.Leela Zero is trained by a distributed effort, which is coordinated at the Leela Zero website. Members of the community provide computing resources by running the client, which generates self-play games and submits them to the server. The self-play games are used to train newer networks. Generally, over 500 clients have connected to the server to contribute resources. The community has provided high quality code contributions as well.Leela Zero finished third at the BerryGenomics Cup World AI Go Tournament in Fuzhou, Fujian, China on 28 April 2018. The New Yorker at the end of 2018 characterized Leela and Leela Zero as "the world's most successful open-source Go engines".In early 2018, another team branched Leela Chess Zero from the same code base, also to verify the methods in the AlphaZero paper as applied to the game of chess. AlphaZero's use of Google TPUs was replaced by a crowd-sourcing infrastructure and the ability to use graphics card GPUs via the OpenCL library. Even so, it is expected to take a year of crowd-sourced training to make up for the dozen hours that AlphaZero was allowed to train for its chess match in the paper.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1049955001},
  file = {/home/zjeffer/Documents/Zotero/storage/IWYWG9ZC/Leela_Zero.html}
}

@misc{LichessOrgOpen,
  title = {Lichess.Org Open Database},
  url = {https://database.lichess.org/},
  urldate = {2022-04-09},
  file = {/home/zjeffer/Documents/Zotero/storage/ZKWLE8MJ/database.lichess.org.html}
}

@misc{masonAnswerHowWe2019,
  title = {Answer to "{{How}} Do We Create a Good Agent That Does Not Outperform Humans?"},
  shorttitle = {Answer to "{{How}} Do We Create a Good Agent That Does Not Outperform Humans?},
  author = {Mason, Oliver},
  year = {2019},
  month = feb,
  journal = {Artificial Intelligence Stack Exchange},
  url = {https://ai.stackexchange.com/a/10433/54037},
  urldate = {2022-05-14},
  file = {/home/zjeffer/Documents/Zotero/storage/9L787QTC/how-do-we-create-a-good-agent-that-does-not-outperform-humans.html}
}

@misc{MasteringGameGo,
  title = {Mastering the Game of {{Go}} with {{Deep Neural Networks}} \& {{Tree Search}}},
  url = {https://www.deepmind.com/publications/mastering-the-game-of-go-with-deep-neural-networks-tree-search},
  urldate = {2022-04-07},
  abstract = {A new approach to computer Go that combines Monte-Carlo tree search with  deep neural networks that have been trained by supervised learning, from  human expert games, and by reinforcement learning, from games of self-play.  The first time ever that a computer program has defeated a human  professional player.},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/8E6TT26Q/mastering-the-game-of-go-with-deep-neural-networks-tree-search.html}
}

@misc{MasteringGameZero,
  title = {Mastering the Game of {{Go}} without {{Human Knowledge}}},
  url = {https://www.deepmind.com/publications/mastering-the-game-of-go-without-human-knowledge},
  urldate = {2022-04-07},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that  learns, *tabula rasa*, superhuman proficiency in challenging domains.  Recently, AlphaGo became the first program to defeat a world champion in  the game of Go. The tree search in AlphaGo evaluated positions and selected  moves using deep neural networks. These neural networks were trained by  supervised learning from human expert moves, and by reinforcement learning  from selfplay. Here, we introduce an algorithm based solely on  reinforcement learning, without human data, guidance, or domain knowledge  beyond game rules. AlphaGo becomes its own teacher: a neural network is  trained to predict AlphaGo's own move selections and also the winner of  AlphaGo's games. This neural network improves the strength of tree search,  resulting in higher quality move selection and stronger self-play in the  next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved  superhuman performance, winning 100-0 against the previously published,  champion-defeating AlphaGo.},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/JQCFTJDT/mastering-the-game-of-go-without-human-knowledge.html}
}

@article{Minimax2022,
  title = {Minimax},
  year = {2022},
  month = mar,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Minimax&oldid=1076761456},
  urldate = {2022-04-05},
  abstract = {Minimax (sometimes MinMax, MM or saddle point) is a decision rule used in artificial intelligence, decision theory, game theory, statistics, and philosophy for minimizing the possible loss for a worst case (maximum loss) scenario.  When dealing with gains, it is referred to as "maximin"\textemdash to maximize the minimum gain.  Originally formulated for n-player zero-sum game theory, covering both the cases where players take alternate moves and those where they make simultaneous moves, it has also been extended to more complex games and to general decision-making in the presence of uncertainty.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1076761456},
  file = {/home/zjeffer/Documents/Zotero/storage/RV43RPZZ/Minimax.html}
}

@misc{MinimaxMonteCarlo,
  title = {Minimax and {{Monte Carlo Tree Search}} - {{Philipp Muens}}},
  url = {https://philippmuens.com/minimax-and-mcts},
  urldate = {2022-04-06},
  abstract = {Understanding the underpinnings of modern Game AIs.},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/I9WAA6KK/minimax-and-mcts.html}
}

@misc{MLMonteCarlo2019,
  title = {{{ML}} | {{Monte Carlo Tree Search}} ({{MCTS}})},
  year = {2019},
  month = jan,
  journal = {GeeksforGeeks},
  url = {https://www.geeksforgeeks.org/ml-monte-carlo-tree-search-mcts/},
  urldate = {2022-04-07},
  abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
  chapter = {Technical Scripter},
  langid = {american},
  file = {/home/zjeffer/Documents/Zotero/storage/XIRFETNI/ml-monte-carlo-tree-search-mcts.html}
}

@misc{moerlandEmailExchangeTuur22,
  title = {Email Exchange between {{Tuur Vanhoutte}} and {{Dr}}. {{Thomas Moerland}}},
  year = {22},
  month = apr,
  collaborator = {Moerland, Thomas}
}

@misc{monkPurposeDirichletNoise2018,
  type = {Forum Post},
  title = {Purpose of {{Dirichlet}} Noise in the {{AlphaZero}} Paper},
  author = {{monk}},
  year = {2018},
  month = apr,
  journal = {Cross Validated},
  url = {https://stats.stackexchange.com/q/322831},
  urldate = {2022-05-14},
  file = {/home/zjeffer/Documents/Zotero/storage/XEBD8XAU/purpose-of-dirichlet-noise-in-the-alphazero-paper.html}
}

@article{MonteCarloTree2022,
  title = {Monte {{Carlo}} Tree Search},
  year = {2022},
  month = apr,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Monte_Carlo_tree_search&oldid=1081107255},
  urldate = {2022-04-06},
  abstract = {In computer science, Monte Carlo tree search (MCTS) is a heuristic search algorithm for some kinds of decision processes, most notably those employed in software that plays board games. In that context MCTS is used to solve the game tree.  MCTS was combined with neural networks in 2016 for computer Go. It has been used in other board games like chess and shogi, games with incomplete information such as bridge and poker, as well as in turn-based-strategy video games (such as Total War: Rome II's implementation in the high level campaign AI). MCTS has also been used in self-driving cars, for example in Tesla's Autopilot software.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1081107255},
  file = {/home/zjeffer/Documents/Zotero/storage/UXBP4BLN/Monte_Carlo_tree_search.html}
}

@misc{mpikybrollLearningRobotTable2012,
  title = {Towards {{Learning Robot Table Tennis}}},
  author = {{mpikybroll}},
  year = {2012},
  month = may,
  url = {https://www.youtube.com/watch?v=SH3bADiB7uQ},
  urldate = {2022-05-14},
  abstract = {This video demonstrates how a Barrett WAM arm uses our mixture of motor primitives (MoMP) algorithm to learn successful hitting movements in table tennis using imitation and reinforcement Learning.}
}

@misc{nbroAnswerAreThere2020,
  title = {Answer to "{{Are}} There Any Applications of Reinforcement Learning Other than Games?"},
  shorttitle = {Answer to "{{Are}} There Any Applications of Reinforcement Learning Other than Games?},
  author = {{nbro}},
  year = {2020},
  month = nov,
  journal = {Artificial Intelligence Stack Exchange},
  url = {https://ai.stackexchange.com/a/24355/54037},
  urldate = {2022-04-13}
}

@misc{NeuralNetworksChessprogramming,
  title = {Neural {{Networks}} - {{Chessprogramming}} Wiki},
  url = {https://www.chessprogramming.org/Neural_Networks#ANNs},
  urldate = {2022-04-07},
  file = {/home/zjeffer/Documents/Zotero/storage/6PJ47EN6/Neural_Networks.html}
}

@misc{openaiDotaLargeScale2019,
  title = {Dota 2 with {{Large Scale Deep Reinforcement Learning}}},
  author = {OpenAI and Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J{\'o}zefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique P. d O. and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
  year = {2019},
  month = dec,
  number = {arXiv:1912.06680},
  eprint = {1912.06680},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1912.06680},
  urldate = {2022-05-14},
  abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/zjeffer/Documents/Zotero/storage/DQM9HEIC/OpenAI et al. - 2019 - Dota 2 with Large Scale Deep Reinforcement Learnin.pdf}
}

@misc{OpenAIFive2019,
  title = {{{OpenAI Five}}},
  year = {2019},
  month = dec,
  journal = {OpenAI},
  url = {https://openai.com/five/},
  urldate = {2022-05-14},
  abstract = {At OpenAI, we've used the multiplayer video game Dota 2 as a research platform for general-purpose AI systems. Our Dota 2 AI, called OpenAI Five, learned by playing over 10,000 years of games against itself. It demonstrated the ability to achieve expert-level performance, learn human\textendash AI cooperation, and},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/TYVEHYQA/five.html}
}

@misc{petarkormushevRobotLearnsFlip2010,
  title = {Robot {{Learns}} to {{Flip Pancakes}}},
  author = {{PetarKormushev}},
  year = {2010},
  month = jul,
  url = {https://www.youtube.com/watch?v=W_gxLKSsSIE},
  urldate = {2022-05-14},
  abstract = {Pancake day special! The video shows a Barrett WAM robot learning to flip pancakes by reinforcement learning. The motion is encoded in a mixture of basis force fields through an extension of Dynamic Movement Primitives (DMP) that represents the synergies across the different variables through stiffness matrices. An Inverse Dynamics controller with variable stiffness is used for reproduction.  For pancake day special, the skill is first demonstrated via kinesthetic teaching, and then refined by Policy learning by Weighting Exploration with the Returns (PoWER) algorithm. After 50 trials, the robot learns that the first part of the task requires a stiff behavior to throw the pancake in the air, while the second part requires the hand to be compliant in order to catch the pancake without having it bounced off the pan. Video credits: -------------------------- Dr. Petar Kormushev http://kormushev.com Affiliation: ---------------------- Advanced Robotics dept. Italian Institute of Technology (IIT) Published paper about this pancake flipping robot experiment: http://kormushev.com/papers/Kormushev... Pancake day special}
}

@misc{pygame,
  title = {{{PyGame}}},
  journal = {PyGame},
  url = {https://www.pygame.org/news},
  urldate = {2022-04-10},
  file = {/home/zjeffer/Documents/Zotero/storage/CFWE8IKH/news.html}
}

@misc{PythonchessChessLibrarya,
  title = {Python-Chess: A Chess Library for {{Python}} \textemdash{} Python-Chess 1.9.0 Documentation},
  url = {https://python-chess.readthedocs.io/en/latest/},
  urldate = {2022-04-08},
  file = {/home/zjeffer/Documents/Zotero/storage/76UIPQ5X/latest.html}
}

@article{ReinforcementLearning2022a,
  title = {Reinforcement Learning},
  year = {2022},
  month = apr,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&oldid=1081715019},
  urldate = {2022-04-14},
  abstract = {Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning. Reinforcement learning differs from supervised learning in not needing labelled input/output pairs be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). Partially supervised RL algorithms can combine the advantages of supervised and RL algorithms. The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques. The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1081715019},
  file = {/home/zjeffer/Documents/Zotero/storage/E4JBNAVW/Reinforcement_learning.html}
}

@misc{rosenbloomTopDogAlphaZero2019,
  title = {Top Dog: {{AlphaZero}} / {{AlphaStar}}},
  shorttitle = {Top Dog},
  author = {Rosenbloom, Lois},
  year = {2019},
  month = mar,
  journal = {Medium},
  url = {https://medium.datadriveninvestor.com/top-dog-alphazero-alphastar-7b2730a6431a},
  urldate = {2022-04-22},
  abstract = {AlphaZero and AlphaStar are two AIs built by Google's DeepMind to play games at a professional level. Building AIs to beat humans (or now\ldots},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/ZTPH54UC/top-dog-alphazero-alphastar-7b2730a6431a.html}
}

@article{SaliencyMap2021,
  title = {Saliency Map},
  year = {2021},
  month = oct,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Saliency_map&oldid=1049691575},
  urldate = {2022-04-16},
  abstract = {In computer vision, a saliency map is an image that highlights the region on which people's eyes focus first. The goal of a saliency map is to reflect the degree of importance of a pixel to the human visual system. For example, in this image, a person first looks at the fort and light clouds, so they should be highlighted on the saliency map.  Saliency maps engineered in artificial or computer vision are typically not the same as the actual saliency map constructed by biological or natural vision.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1049691575},
  file = {/home/zjeffer/Documents/Zotero/storage/MQ8XIN5E/Saliency_map.html}
}

@article{silverMasteringChessShogi2017a,
  title = {Mastering {{Chess}} and {{Shogi}} by {{Self-Play}} with a {{General Reinforcement Learning Algorithm}}},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.01815 [cs]},
  eprint = {1712.01815},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1712.01815},
  urldate = {2022-02-01},
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/zjeffer/Documents/Zotero/storage/ZRRMASQ3/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf;/home/zjeffer/Documents/Zotero/storage/CIJ93UYW/1712.html}
}

@misc{slaterAnswerAreThere2017,
  title = {Answer to "{{Are}} There Any Applications of Reinforcement Learning Other than Games?"},
  shorttitle = {Answer to "{{Are}} There Any Applications of Reinforcement Learning Other than Games?},
  author = {Slater, Neil},
  year = {2017},
  month = jul,
  journal = {Artificial Intelligence Stack Exchange},
  url = {https://ai.stackexchange.com/a/3730/54037},
  urldate = {2022-04-13},
  file = {/home/zjeffer/Documents/Zotero/storage/SRCK4F9F/are-there-any-applications-of-reinforcement-learning-other-than-games.html}
}

@article{SolvedGame2022,
  title = {Solved Game},
  year = {2022},
  month = apr,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Solved_game&oldid=1081592996},
  urldate = {2022-04-30},
  abstract = {A solved game is a game whose outcome (win, lose or draw) can be correctly predicted from any position, assuming that both players play perfectly. This concept is usually applied to abstract strategy games, and especially to games with full information and no element of chance; solving such a game may use combinatorial game theory and/or computer assistance.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1081592996},
  file = {/home/zjeffer/Documents/Zotero/storage/HSBTS5DV/Solved_game.html}
}

@article{SolvingChess2022,
  title = {Solving Chess},
  year = {2022},
  month = feb,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Solving_chess&oldid=1071082843},
  urldate = {2022-04-30},
  abstract = {Solving chess means finding an optimal strategy for the game of chess, that is, one by which one of the players (White or Black) can always force a victory, or either can force a draw (see solved game). It also means more generally solving chess-like games (i.e. combinatorial games of perfect information), such as Capablanca chess and infinite chess. According to Zermelo's theorem, a determinable optimal strategy must exist for chess and chess-like games. In a weaker sense, solving chess may refer to proving which one of the three possible outcomes (White wins; Black wins; draw) is the result of two perfect players, without necessarily revealing the optimal strategy itself (see indirect proof).No complete solution for chess in either of the two senses is known, nor is it expected that chess will be solved in the near future. There is disagreement on whether the current exponential growth of computing power will continue long enough to someday allow for solving it by "brute force", i.e. by checking all possibilities. Progress to date is extremely limited; there are tablebases of perfect endgame play with a small number of pieces, and several reduced chess-like variants have been solved at least weakly. Calculated estimates of game tree complexity and state-space complexity of chess exist which provide a bird's eye view of the computational effort that might be required to solve the game.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1071082843},
  file = {/home/zjeffer/Documents/Zotero/storage/CA5ZWMJQ/Solving_chess.html}
}

@article{StockfishChess2022,
  title = {Stockfish (Chess)},
  year = {2022},
  month = mar,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Stockfish_(chess)&oldid=1079146589},
  urldate = {2022-03-28},
  abstract = {Stockfish is a free and open-source chess engine, available for various desktop and mobile platforms. It can be used in chess software through the Universal Chess Interface. Stockfish is consistently ranked first or near the top of most chess-engine rating lists and is the strongest CPU chess engine in the world. It has won the Top Chess Engine Championship 11 times. Stockfish is developed by Marco Costalba, Joona Kiiski, Gary Linscott, Tord Romstad, St\'ephane Nicolet, Stefan Geschwentner, and Joost VandeVondele, with many contributions from a community of open-source developers. It is derived from Glaurung, an open-source engine by Tord Romstad released in 2004.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1079146589},
  file = {/home/zjeffer/Documents/Zotero/storage/FPIS5XAZ/Stockfish_(chess).html}
}

@misc{StockfishChessprogrammingWiki,
  title = {Stockfish - {{Chessprogramming}} Wiki},
  url = {https://www.chessprogramming.org/Stockfish},
  urldate = {2022-05-07},
  file = {/home/zjeffer/Documents/Zotero/storage/4L4V4AYL/Stockfish.html}
}

@misc{StockfishOpenSource,
  title = {Stockfish - {{Open Source Chess Engine}}},
  url = {https://stockfishchess.org/},
  urldate = {2022-05-07}
}

@misc{surmenokContextualBanditsReinforcement2017,
  title = {Contextual {{Bandits}} and {{Reinforcement Learning}}},
  author = {Surmenok, Pavel},
  year = {2017},
  month = oct,
  journal = {Medium},
  url = {https://towardsdatascience.com/contextual-bandits-and-reinforcement-learning-6bdfeaece72a},
  urldate = {2022-04-13},
  abstract = {If you develop personalization of user experience for your website or an app, contextual bandits can help you. Using contextual bandits\ldots},
  langid = {english},
  file = {/home/zjeffer/Documents/Zotero/storage/W7M4MUFB/contextual-bandits-and-reinforcement-learning-6bdfeaece72a.html}
}

@article{TensorProcessingUnit2022,
  title = {Tensor {{Processing Unit}}},
  year = {2022},
  month = mar,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Tensor_Processing_Unit&oldid=1077688658},
  urldate = {2022-04-07},
  abstract = {Tensor Processing Unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google specifically for neural network machine learning, particularly using Google's own TensorFlow software. Google began using TPUs internally in 2015, and in 2018 made them available for third party use, both as part of its cloud infrastructure and by offering a smaller version of the chip for sale.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1077688658},
  file = {/home/zjeffer/Documents/Zotero/storage/HUNP7D9R/Tensor_Processing_Unit.html}
}

@misc{ThomasMoerlandPostdoc,
  title = {{Thomas Moerland \textendash{} Postdoc, Leiden University, The Netherlands}},
  url = {https://thomasmoerland.nl/},
  urldate = {2022-04-13},
  langid = {dutch},
  file = {/home/zjeffer/Documents/Zotero/storage/G56YLFZN/thomasmoerland.nl.html}
}

@article{TopChessEngine2022,
  title = {Top {{Chess Engine Championship}}},
  year = {2022},
  month = mar,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Top_Chess_Engine_Championship&oldid=1077985240},
  urldate = {2022-04-12},
  abstract = {Top Chess Engine Championship, formerly known as Thoresen Chess Engines Competition  (TCEC or nTCEC), is a computer chess tournament that has been run since 2010. It was organized, directed, and hosted by Martin Thoresen until the end of Season 6; from Season 7 onward it has been organized by Chessdom. It is often regarded as the Unofficial World Computer Chess Championship because of its strong participant line-up and long time-control matches on high-end hardware, giving rise to very high-class chess. The tournament has attracted nearly all the top engines compared to the World Computer Chess Championship. After a short break in 2012, TCEC was restarted in early 2013 (as nTCEC) and is currently active (renamed as TCEC in early 2014) with 24/7 live broadcasts of chess matches on its website.  Since season 5, TCEC has been sponsored by Chessdom Arena.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1077985240},
  file = {/home/zjeffer/Documents/Zotero/storage/2AMB254S/Top_Chess_Engine_Championship.html}
}

@misc{zjefferChessboardZjeffer2022,
  title = {Chess-Board Zjeffer},
  author = {{zjeffer}},
  year = {2022},
  month = jan,
  url = {https://github.com/zjeffer/chess-board},
  urldate = {2022-04-10},
  abstract = {A chess board library for presenting game positions},
  copyright = {GPL-3.0}
}

@misc{zjefferChessdeeprlcpp2022,
  title = {Chess-Deep-Rl-Cpp},
  author = {{zjeffer}},
  year = {2022},
  month = apr,
  url = {https://github.com/zjeffer/chess-deep-rl-cpp},
  urldate = {2022-04-10},
  abstract = {C++ version of my chess-deep-rl project. WIP},
  keywords = {ai,alphazero,artificial-intelligence,chess,chess-engine,deep-learning,deep-reinforcement-learning,machine-learning,mcts,neural-networks,reinforcement-learning}
}

@misc{zjefferChessEngineDeep2022,
  title = {Chess Engine with {{Deep Reinforcement}} Learning},
  author = {{zjeffer}},
  year = {2022},
  month = feb,
  url = {https://github.com/zjeffer/chess-deep-rl},
  urldate = {2022-04-09},
  abstract = {Research project: create a chess engine using Deep Reinforcement Learning},
  copyright = {GPL-3.0},
  keywords = {ai,alphazero,artificial-intelligence,chess,chess-engine,deep-learning,deep-reinforcement-learning,machine-learning,mcts,neural-network,neural-networks,reinforcement-learning}
}

@misc{zjefferHowCanAlphaZero2022,
  type = {Forum Post},
  title = {How Can {{AlphaZero}} Be Used in Other Industries besides Gaming?},
  author = {{zjeffer}},
  year = {2022},
  month = apr,
  journal = {Artificial Intelligence Stack Exchange},
  url = {https://ai.stackexchange.com/q/35193/54037},
  urldate = {2022-05-14},
  file = {/home/zjeffer/Documents/Zotero/storage/RT6BKXVU/how-can-alphazero-be-used-in-other-industries-besides-gaming.html}
}

@misc{zjefferWhatCanWe2022,
  type = {Forum Post},
  title = {What Can We Learn from {{AlphaZero}} in the Development towards {{AGI}}?},
  author = {{zjeffer}},
  year = {2022},
  month = apr,
  journal = {Artificial Intelligence Stack Exchange},
  url = {https://ai.stackexchange.com/q/35194/54037},
  urldate = {2022-05-14},
  file = {/home/zjeffer/Documents/Zotero/storage/ZVNAIRMY/what-can-we-learn-from-alphazero-in-the-development-towards-agi.html}
}


