\documentclass{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                   %
%              Template by Tuur Vanhoutte           %
% https://github.com/zjeffer/howest-thesis-template %
%                                                   % 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%% Package importing %%%%%
\usepackage[english]{babel}
\usepackage[margin=2.5cm]{geometry}
\usepackage[table]{xcolor}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage[parfill]{parskip}
\usepackage{eso-pic}
\usepackage{titlesec}
\usepackage{titletoc}
\usepackage{tcolorbox}
\usepackage{enumitem}
\usepackage{changepage}
\usepackage{afterpage}
\usepackage{totalcount}


\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}


\usepackage{adjustbox}
\usepackage{tabularx,ragged2e,booktabs}

\definecolor{lightgray}{gray}{0.9}

\let\oldtabularx\tabularx
\renewcommand*{\tabularx}{\rowcolors{1}{white}{lightgray}\oldtabularx}

\newcolumntype{L}{>{\RaggedRight\arraybackslash}X}


\usepackage{bookmark}
\usepackage{csquotes}
\usepackage[style=ieee]{biblatex}
\addbibresource{sources.bib}


%%%%% Change fonts here %%%%%
\usepackage[T1]{fontenc}
\usepackage{helvet}
% \renewcommand{\familydefault}{\sfdefault}

\graphicspath{{img/}}

%%%%% \theorem environment %%%%%
\usepackage{amssymb}
\newtheorem{theorem}{Definition}[section]
\newenvironment{thmenum}
 {\begin{enumerate}[label=\upshape\bfseries(\roman*)]}
 {\end{enumerate}}


%%%%% Styled code block %%%%%
\usepackage{minted}
\setminted{frame=single,framesep=3pt,linenos}
\usepackage{upquote}
\usepackage{color}

%%%%% TOC styling %%%%%
\makeatletter
\renewcommand\tableofcontents{%
  \null\hfill\textbf{\Huge\contentsname}\hfill\null\par
  \vline\noexpand\rule{\textwidth}{1pt}%
  \@mkboth{\MakeUppercase\contentsname}{\MakeUppercase\contentsname}%
  \@starttoc{toc}%
}
\makeatother

\begin{document}

%%%%% title page %%%%%
\begin{titlepage}
    \newgeometry{margin=0cm}
    \begin{figure}[H]
        %%%%% Change the cover image here %%%%%
        \centering
        \includegraphics[width=18.5cm,height=18.5cm]{img/titlepage-cover.png}
        \caption{Image source: Rosenbloom, Lois {\cite{rosenbloomTopDogAlphaZero2019}}}
    \end{figure}
    \begin{adjustwidth}{1.5cm}{1.5cm}

    \vspace{0.5em}

    \MakeUppercase{\Huge\textbf{How to create a chess engine with Deep Reinforcement Learning}}

    \MakeUppercase{\Large\textit{A critical look at DeepMind's AlphaZero}}

    \vspace{1em}

    \MakeUppercase{Internal promotor: Wouter Gevaert}

    \MakeUppercase{External promotor: Tom Vandecaveye}

    \vspace{1em}

    \MakeUppercase{\small{Research conducted by}}

    \MakeUppercase{\Large\textbf{{Tuur Vanhoutte}}}

    \MakeUppercase{\small{for obtaining a bachelor's degree in}}

    \MakeUppercase{\Large{\textbf{{Multimedia \& Creative Technologies}}}}

    \MakeUppercase{Howest | 2021-2022}
    \end{adjustwidth}
\end{titlepage}

\newgeometry{margin=2.5cm}

\newpage
\thispagestyle{empty}
\mbox{}
\newpage

%%%%% Set pagenumbering off %%%%%
\pagenumbering{arabic}
\thispagestyle{empty}
%%%%% Preface %%%%%
\section*{Preface}
\addcontentsline{toc}{section}{Preface}

This bachelor thesis is the conclusion to the bachelor's program Multimedia \& Creative Technologies at Howest College
West Flanders in Kortrijk, Belgium. The program teaches students a wide range of skills in the field of 
computer science, with a focus on creativity and the Internet of Things. From the second year on, students can choose 
between four different modules:

\begin{enumerate}
    \item \textbf{AI Engineer}
    \item \textbf{Smart XR Developer}
    \item \textbf{Next Web Developer}
    \item \textbf{IoT Infrastructure Engineer}
\end{enumerate}

This bachelor thesis was made under the \textbf{AI Engineer} module.
The subject of the thesis is a critical look at the results of my research project 
in the previous semester. The goal of the project was to create a chess engine in Python with
deep reinforcement learning based on DeepMind's AlphaZero algorithm. 
As I have been playing chess for most of my life, I was naturally very interested in the workings of AlphaZero
and why it played so well against the more conventional `StockFish' engine. 

I will explain the research I needed to create it, the technical details of how I programmed the
chess engine and I will reflect on the results of the project. To do this, I will contact 
multiple people and communities familiar with the field of reinforcement learning to get a better understanding of the
impact of this research on society. Based on this, I will advise people and companies who 
wish to implement similar algorithms.

I would like to thank Wouter Gevaert for his enthusiastic support in the creation of my research project 
and this thesis. I also want to thank the other teachers at Howest Kortrijk, who enthusiastically shared their knowledge
and expertise in programming and AI.

Furthermore, I'm grateful to my external promotor, Tom Vandecaveye, who has agreed to read and grade
this thesis as an unbiased party. Mr. Vandecaveye is an employee of dotOcean, where I did my internship.

Additionally, I would like to thank my sister for professionally proofreading this thesis for 
errors in grammar and spelling.
Finally, I would like to thank my parents for giving me the chance to have a good education, and 
the motivation to get the best I can out of my studies.

\vspace{3em}

\begin{center}
    \textbf{Tuur Vanhoutte}, 30$^{\text{th}}$ May 2022
\end{center}

\newpage
\thispagestyle{empty}
\mbox{}
\newpage

%%%%% Abstract %%%%%
\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}


% Samenvatting of abstract (mag in het Engels): MAX 1 halve A4-pagina: 
% Je beantwoordt in de samenvatting kort en bondig een viertal vragen: 
% X Wat is de onderzoeksvraag?
% X Wat was jouw onderzoek? 
% X Welke elementen spelen een grote rol (zowel positief als negatief) bij de evaluatie van het onderzoek? 
% X Welke elementen zijn belangrijk bij jouw advies?
% X Het besluit wordt kort samengevat

This bachelor thesis answers the question: `How to create a chess engine using deep reinforcement learning?'.
It explains the differences between conventional chess engines and chess engines that use deep reinforcement learning, and
specifically tries to recreate the results of AlphaZero, the chess engine by DeepMind, in Python on consumer hardware. 

The technical research shows what was needed to create my implementation using Python and TensorFlow. 
It shows how to program the chess engine, how to build the neural network, and how to train and evaluate the network.
During the creation of this chess engine, it was crucial to create an enormous amount of data through self-play.
This plays a big role in the critical evaluation of the project. 

The thesis contains a reflection on the results of my research project, which proposes a solution to the problem of
having to create a high number of games through self-play. It also reflects on the impact of this research on society, and the
viability of this type of artificial intelligence in the future. 
It follows with advice to people and companies who wish to implement similar algorithms, warning that while this type of 
algorithm can be applied to solve many problems, the computational power necessary to successfully implement it is very high. 

Finally, the conclusion offers a definitive answer to the research question, based on the previous sections.
It explains that it is definitely possible to create a chess engine with deep reinforcement learning, as proven by DeepMind,
but it is almost impossible to do so on consumer hardware. 

\newpage
\thispagestyle{empty}
\mbox{}
\newpage

%%%%% Table of contents %%%%%
\addcontentsline{toc}{section}{Contents}
\tableofcontents
\newpage

%%%%% List of figures %%%%%
\section*{List of figures}
\addcontentsline{toc}{section}{List of figures}
\renewcommand{\listfigurename}{}
\listoffigures

\section*{List of tables}
\addcontentsline{toc}{section}{List of tables}
\renewcommand{\listtablename}{}
\listoftables

%%%%% List of abbreviations %%%%%
\newpage
\section*{List of abbreviations}
\addcontentsline{toc}{section}{List of abbreviations}


\begin{table}[h!]
    \begin{tabularx}{\textwidth}{@{}LL@{}}
    \toprule
    \multicolumn{1}{c}{\textbf{Abbreviation}} & \multicolumn{1}{c}{\textbf{Explanation}} \\ 
    \midrule 
    A2C & Advantage Actor-Critic Algorithm \\ \addlinespace
    A3C & Asynchronous Advantage Actor-Critic Algorithm \\ \addlinespace
    AGI & Artificial General Intelligence \\ \addlinespace
    AI & Artificial Intelligence  \\ \addlinespace 
    ASIC & Application-specific Integrated Circuit \\ \addlinespace
    CCC & The Computer Chess Club \\ \addlinespace
    CPU & Central Processing Unit \\ \addlinespace
    CTO & Chief Technology Officer \\ \addlinespace
    DQN & Deep Q-Network \\ \addlinespace
    FEN & Forsyth-Edwards Notation \\ \addlinespace
    GB & Gigabyte \\ \addlinespace
    GCP & Google Cloud Platform \\ \addlinespace
    GPU & Graphics Processing Unit \\ \addlinespace
    gRPC & Google Remote Procedure Call \\ \addlinespace
    GUI & Graphical User Interface \\ \addlinespace
    IoT & Internet of Things \\ \addlinespace
    lc0 & Leela Chess Zero \\ \addlinespace
    LIME & Local Interpretable Model-Agnostic Explanations \\ \addlinespace
    LSTM & Long Short-Term Memory \\ \addlinespace
    MCTS & Monte Carlo Tree Search \\ \addlinespace
    NN & Neural Network \\ \addlinespace
    PGN & Portable Game Notation \\ \addlinespace
    RAM & Random Access Memory \\ \addlinespace
    ReLU & Rectified Linear Unit \\ \addlinespace
    SARSA & State-Action-Reward-State-Action \\ \addlinespace
    SHAP & Shapley Value \\ \addlinespace
    SVG & Scalable Vector Graphics \\ \addlinespace
    TPU & Tensor Processing Unit \\ \addlinespace
    UCB & Upper Confidence Bound \\ \addlinespace
    VRAM & Video RAM \\ \addlinespace
    XR & Extended Reality \\ \addlinespace
    \bottomrule
    \end{tabularx} 
\end{table}

% TODO: aanvullen

%%%%% Glossary %%%%%
\newpage
\section*{Glossary}
\addcontentsline{toc}{section}{Glossary}

\begin{table}[h!]
    \begin{tabularx}{\textwidth}{@{}LL@{}}
    \toprule
    \multicolumn{1}{c}{\textbf{Term}} & \multicolumn{1}{c}{\textbf{Definition}} \\ 
    \midrule
    % term & definition \\ \addlinespace
    Branching factor & The average number of actions that can be made from any state in the environment. \\ \addlinespace
    Deep reinforcement learning & A type of reinforcement learning that employs a neural network to learn from experience. \\ \addlinespace
    Deterministic selection & Selecting an action based on the largest value of all state-action pairs. \\ \addlinespace 
    Elo rating & A rating system for comparing the skill level of players in zero-sum games like chess \cite{EloRatingSystem2022}. \\ \addlinespace
    Search tree & A tree data structure to represent the future possible states in an environment. \\ \addlinespace
    Self-play & Deploying an algorithm that plays against a copy of itself, to improve itself or to measure performance. \\ \addlinespace
    Stochastic selection & Selecting an action based on a probability distribution. \\ \addlinespace
    \bottomrule
    \end{tabularx} 
\end{table}


%%%%% Introduction %%%%%
\newpage
\setcounter{section}{0}
\section{Introduction}

% Doel: In de inleiding beschrijf je ook hoe jouw bachelorproef in elkaar steekt. Een krachtige heldere inleiding 
% zorgt ervoor dat je de lezer voor je wint en hij/zij sneller de rest van jouw document zal gaan lezen. 
% In de inleiding introduceer je de onderzoeksvraag. Je vermeldt de achtergrond of bestaande situatie. Je licht toe 
% waarom de onderzoeksvraag voor jou/jouw stagebedrijf relevant is. Ook eventuele deelvragen worden 
% nauwgezet omschreven. 
% De inleiding omschrijft ook de gebruikte onderzoeksmethode. Je legt uit waar, wanneer, met wie en hoe je het 
% onderzoek gaat doen. 
% Je kunt alvast gebruikmaken van één of meerdere standaardzinnen: 
% - De data voor dit onderzoek zijn verzameld door... 
% - Vijf stukken worden onderzocht, die allemaal... 
% - De onderzoeksgegevens in deze bachelorproef zijn afkomstig uit vier belangrijke bronnen, namelijk... 
% - Door kwalitatieve methoden te gebruiken probeer ik... uiteen te zetten/uit te lichten. 
% - De studie is uitgevoerd in de vorm van een enquête, waarbij data zijn verzameld via... 
% - De methode die in deze studie gebruikt is, is een gemengde aanpak gebaseerd op...

Chess is not only one of the oldest and most popular board games in the world, it is also a breeding ground for
complex algorithms, and more recently, machine learning. Chess is theoretically a deterministic game, i.e.
no information is hidden from either player and every position has a calculable set of possible moves.
It is not a `solved' game, which means the outcome of any position can not always be correctly predicted, only estimated. 
Because the branching factor of chess is about 35 to 38 moves \cite{BranchingFactorChessprogramming}, 
meaning that in every position an average of 35 to 38 actions are possible, 
this estimation requires an enormous number of calculations.

Throughout the entire history of computer science, researchers have continuously tried to find better
ways to calculate whether a position is winning or losing. The most famous example is the StockFish 
engine \cite{StockfishChess2022}, which uses the minimax algorithm with alpha-beta pruning to calculate the best move.

Recently, researchers at Google DeepMind have developed a new algorithm called AlphaZero \cite{AlphaZero2022}.
AlphaZero was made using deep reinforcement learning, by playing millions of games against itself and 
using those games as training data for the neural network. With help from Google's AI-specialised hardware,
AlphaZero managed to play chess better than StockFish after only four hours of training.
This thesis explores the concept of AlphaZero, how to create a chess engine based on it, and the impact of 
the algorithm on both the world of chess and beyond.

Research has been conducted by investigating what is needed to recreate the results of AlphaZero
by programming a simple implementation using Python and TensorFlow Keras. This was done as part of a research project
between November 2021 and February 2022. The code was written with lots of trial and error, as DeepMind released 
very little information about the detailed workings of the algorithm. It also only released a simple version of 
the algorithm in pseudocode, so it isn't possible to directly compare AlphaZero with other chess engines.
Because AlphaZero was trained on supercomputers, I wanted to investigate where its flaws lie by 
implementing it in Python on consumer hardware.


%%%%% Add Howest background %%%%%
\AddToShipoutPicture{
    \ifnum\value{page}>0
        % except last page
        \ifnum\value{page}<\pagetotal
            \AtPageLowerLeft{
            \raisebox{3\baselineskip}{\makebox[0.25\paperwidth]{
                \begin{minipage}{21cm}\centering
                    \includegraphics{img/background.png}
                \end{minipage}}}
            }
        \fi
    \fi
}

%%%%% Research %%%%%
\newpage
\section{Research}

%%%%%%%%%%%%% 
% 
% Answer the theoretical questions:
%
% [X] Hoe kan ik het Monte Carlo Tree Search algoritme gebruiken om de beste zetten te selecteren?
% [X] Welke voordelen biedt deep reinforcement learning bij schaken?
% [X] Hoe ziet de architectuur van het neurale netwerk er uit?
% [X] Hoe werd deep reinforcement learning reeds gebruikt bij andere spellen?
% [X] Hoe werken huidige schaakcomputers die geen neurale netwerken gebruiken?
%
%%%%%%%%%%%%%

\subsection{Chess}

Chess is a two-player strategy board game \cite{Chess2022a}. The game is played on square board of 64 squares arranged in an eight-by-eight grid, 
with sixteen pieces for each player. One player plays the white pieces, and the other plays the black pieces.
There are six types of pieces in chess: pawns, rooks, knights, bishops, queens, and kings. Every player gets eight pawns,
two rooks, two knights, two bishops, one queen and one king. Every one of these pieces has a specific set of possible actions. 

Pieces can be captured by the opponents pieces, after which the captured piece will be removed from the board, and the opponent's piece
will be placed at the captured piece's position. The goal of the game is to checkmate the opponent.

\subsubsection{Game setup}

At the start of a chess game, the pieces are placed on the board in the following positions:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{img/chess-initial.png}
    \caption{Chessboard at the start of the game \cite{ChessBoardEditor}}
\end{figure}

From the bottom left: rook, knight, bishop, queen, king, bishop, knight, rook.
The second row is filled with 8 pawns. The opponent places the same pieces on the other side of the board.
In chess variants like Fischer random chess, the pieces on the first and last rows are shuffled randomly \cite{FischerRandomChess2022}.

\subsubsection{Movement}

Pawns can normally only move forward one square at a time, but if the pawn is in its initial position, the player can choose to move them two squares forward.
To capture pieces, pawns must take diagonally. It can only do this when an opponent's piece is diagonally one square removed from the pawn.
When a pawn reaches the other side of the board, it can choose to `promote' itself to a queen, rook, bishop, or knight, i.e. the pawn is 
replaced by the chosen piece. `En passant' is a special pawn move that is only possible when the player moves a pawn two squares forward. 
If the pawn is then adjacent to one of the opponent's pawns, the opponent can choose to capture that pawn as if it had only moved one square forward.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{img/en-passant.png}
    \caption{En passant: the black pawn can capture the adjacent white pawn, because it moved two squares}
\end{figure}

The rook can move any number of squares vertically or horizontally, given there are no pieces blocking its way.
The bishop can do the same, but diagonally. The queen is a combination of the rook and bishop: it can move vertically, diagonally, and horizontally.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.32\textwidth]{img/rook.png}
    \includegraphics[width=0.32\textwidth]{img/bishop.png}
    \includegraphics[width=0.32\textwidth]{img/queen.png}
    \caption{Rook, bishop and queen moves}
\end{figure}


The knight moves in an L-shape: a knight move consists of moving one square vertically, and two horizontally, or vice versa.
The knight is the only chess piece that can leap over other pieces.

The king can move just like the queen, but only one square at a time. 
The king can also `castle' - a special move that gets the king to safety by moving it two squares towards one 
of the rooks, and moving the rook to the square that the king has crossed \cite{Castling2022}.


\subsubsection{Check and checkmate}

Attacking the king is called a `check'. When this happens, the opponent must either move the king out of the way, or
stop the attack by capturing the attacking piece or blocking the attack with one of its own pieces. 
When the player can not stop the attack, the player loses the game. This is called `checkmate'.
The player can never end their move if their king is in check. This would be an `illegal move'.

When it's the player's turn but have no legal moves available, and they are not checked, the game is drawn.
This is called `stalemate'.

\subsection{Chess engines}

According to Wikipedia \cite{ChessEngine2022}, a chess engine is a computer program that analyzes 
positions in chess or chess variants, and generates a list of moves that it regards as strongest.
Given any chess position, the engine will estimate the winner of that position based on the strength 
of the possible future moves up to a certain depth. The strength of a chess engine is often determined by
the number of positions, both in depth and breadth, that the engine can evaluate. 

This means that with time, as computational power increases, chess engines will keep getting stronger.


\subsection{How do traditional chess engines work?}

\subsubsection{The minimax algorithm}

Contemporary chess engines, like StockFish \cite{StockfishChess2022}, use a variant of the minimax algorithm that employs alpha-beta pruning.

The minimax algorithm \cite{Minimax2022} is a general algorithm usable in many applications, ranging from artificial intelligence to 
decision theory and game theory. The algorithm tries to minimize the maximum amount of loss. In chess, this means 
that the engine tries to minimize the possibility for the worst-case scenario, i.e. the opponent checkmating the player. 
Alternatively, for games where the player needs to maximize a score, the algorithm is called maximin: maximizing the minimum gain. 

Minimax recursively creates a search tree \cite{eppesHowComputerizedChess2019}, with chess positions as nodes and chess moves as edges between the nodes. 
Each node has a value that represents the strength of the position for the current player. 
At the start of the algorithm, the tree only consists of a root node that represents the current position. 
It then explores the tree in a depth-first manner by continuously choosing random legal moves, creating nodes and edges in the process.

This means that it will traverse the tree vertically until a certain depth is reached:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{img/depth-vs-breadth.png}
    \caption{Depth-First search vs Breadth-First search \cite{eppesHowComputerizedChess2019}}
\end{figure}

When that happens, that leaf node's position is evaluated and its value is returned
upwards to the parent node. The parent node looks at all of its children's values, 
and receives the maximum value when playing white, and the minimum value
when playing black. 

This repeats until the root node receives a value: the strength of the current position.

\subsubsection{The evaluation function}

The value estimation of leaf nodes is done by an evaluation function \cite{EvaluationFunction2022} written specifically
for the game. This function can differ from engine to engine, and is usually written with the
help from chess grandmasters. 

\subsubsection{Pseudocode}

The algorithm is recursive, i.e. it calls itself with different arguments, depending
on which player's turn it is. In chess, white wants to maximize the score, and 
black wants to minimize it \cite{Minimax2022}. 

\begin{minted}{c}
function  minimax(node, depth, maximizingPlayer) is
    if depth = 0 or node is a terminal node then
        return the heuristic value of node
    if maximizingPlayer then
        value := - inf
        for each child of node do
            value := max(value, minimax(child, depth - 1, FALSE))
        return value
    else (* minimizing player *)
        value := + inf
        for each child of node do
            value := min(value, minimax(child, depth - 1, TRUE))
        return value
\end{minted}

Calling the function:

\begin{minted}{c}
// origin = node to start
// depth = depth limit
// maximizingPlayer = TRUE if white, FALSE if black
minimax(origin, depth, TRUE)
\end{minted}

\subsubsection{Alpha-beta pruning}

Because the number of nodes necessary to get a good estimate of the strength of a position
is so high, the algorithm needs to be optimized. 
Alpha-beta pruning \cite{AlphaBetaPruning2022} aims to reduce the number of nodes that need to be explored by minimax.
It does this by cutting off branches in the search tree that lead to worse outcomes.

Say you're playing the white pieces. You want to minimize your maximum loss, which means 
you want to make sure that black's score is as low as possible. 
Minimax assumes that the opponent will play the best possible move. If one of white's possible moves
leads to a position where black gets a big advantage, it will eliminate that branch of the search tree.
As a result, the number of nodes to explore is greatly reduced, while retaining an accurate estimate 
of the strength of the position.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{img/AB-pruning.png}
    \caption{Example of alpha-beta pruning in minimax}
\end{figure}

In the above example, white has a value of 6 in the root node. 
If white plays the move on the right that leads to a position with a value of 5, 
the next move black wants to play will be the one that leads to the position 
with the lowest possible value. Therefore, it will never play the move
that leads to the position with a value of 8, as that would be a winning position for
white. This means that whole branch can be pruned.

\subsection{Monte Carlo Tree Search}

The biggest problem with minimax algorithms that use a depth limit is the dependency on the evaluation function.
If the evaluation function makes incorrect or suboptimal estimations, the algorithm will suggest bad moves. 
Developers of contemporary chess engines like StockFish continuously try to improve this function. 
Since 2020, StockFish has been using a sparse and shallow neural network as its evaluation function. 
This neural network is still trained using supervised learning, not (deep) reinforcement learning.

Using alpha-beta pruning can also bring about some problems. Say the player can sacrifice a piece
to get a huge advantage later in the game. The algorithm might cut off the branch and never explore that winning line, 
because it considers the sacrifice a losing position \cite{MinimaxMonteCarlo}. 

Monte Carlo Tree Search (MCTS) \cite{MonteCarloTree2022} is a search algorithm that can be used to mitigate these problems.
MCTS approximates the value of a position by creating a search tree using random exploration of the most promising moves.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\textwidth]{img/MCTS-steps.png}
    \caption{The 4 steps of the MCTS algorithm \cite{MonteCarloTree2022}}
\end{figure}

To create this search tree for a certain position, MCTS will run the following algorithm hundreds of times, 
consisting of four steps. 
Every execution of the algorithm is called an MCTS \textbf{simulation}. 
This is not to be confused with the third step of the algorithm, which is also called \textit{simulation}. 

\subsubsection{Selection}

Starting from the root node, select a child node based on a formula of your choice.
Most implementations of MCTS use some variant of Upper Confidence Bound (UCB) \cite{MLMonteCarlo2019}
Keep selecting nodes until a node has been reached that has not been visited (= `expanded') before. We call this a leaf node.
If the root node is a leaf node, we immediately proceed to the next step.


\subsubsection{Expansion}

If the selected leaf node is a terminal node (the game ends), proceed to the backpropagation step.
When it isn't, create a child node for every possible action that can be taken from the selected node.


\subsubsection{Simulation / Rollout}

Choose a random child node that was expanded in the previous step.
By only choosing random moves, simulate the rest of the game from that child node's position.

\subsubsection{Backpropagation}

Return the simulation's result up the tree.
Every node tracks the number of times it has been visited, and the number of times it has led to a win.

For chess, this algorithm is very inefficient, because of its necessity to simulate 
an entire game of chess in the third step of every simulation. 
To calculate the value of only one position, there would need to be hundreds of these simulations to get a good estimation.
Therefore, the selection formula needs to be chosen carefully since it is important to select nodes in a way that
balances exploration and exploitation.

\subsection{Go}

Go is a Chinese two-player strategy board game that uses white and black stones as playing pieces \cite{GoGame2022}.
It is played on a rectangular grid of (usually) nineteen by nineteen lines. The rules are relatively simple, but due to 
its extremely high dimensional state space, Go has been a very popular playground for AI research similarly to chess.
Go's much larger branching factor compared to chess makes it very difficult to evaluate a position using 
traditional methods like minimax with alpha-beta pruning.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{img/go.jpg}
    \caption{Go board \cite{GoGame2022}}
\end{figure}

\subsubsection{AlphaGo}

In 2014, DeepMind Technologies \cite{DeepMind2022}, a subsidiary of Google, 
started developing a new algorithm called AlphaGo to play Go \cite{AlphaGo2022a}. 
Previously, the strongest Go engines were only good enough to win against amateur Go players \cite{AlphaGo}.
The algorithm used a combination of the MCTS algorithm and a deep neural network to evaluate positions. 

AlphaGo was built \cite{AlphaGo, MasteringGameGo}  by first training a neural network with supervised learning by using data from human games.
The weights of that network were then copied to a new reinforcement learning network. That network was used to 
create a training set through self-play. A training set was created by playing against itself and every move 
recording the current board state, the  moves the network considered, and the eventual winner of the game.
That training set was then used to train the reinforcement learning network. A separate network (the value network) 
was used to estimate the value of a position. 

\subsubsection{AlphaGo Zero}

Because AlphaGo still used some amateur games to learn from, the next step was creating a version of AlphaGo
that learned completely from scratch. That is why DeepMind developed AlphaGo Zero \cite{AlphaGoZero2022}.
AlphaGo Zero uses a different kind of network than AlphaGo. Instead of using two separate networks, 
it will combine the two networks into one with two outputs: a policy output and a value output.
It's also using different layers: residual layers instead of convolutional layers \cite{MasteringGameZero}. 

\subsection{AlphaZero}

AlphaZero is a generalized version of AlphaGo Zero, created to master the games of chess, shogi (`Japanese chess'), and Go \cite{AlphaZero2022, silverMasteringChessShogi2017a}. 
For chess, AlphaZero was evaluated against StockFish version 8 by playing a thousand games with three hours per player, plus 
fifteen seconds per move. It won 155 times, lost 6 times and the remaining games were drawn. 
AlphaZero uses a single neural network with two outputs, just like AlphaGo Zero. 


\subsubsection{Neural network input}

The input to the network represents the current state of the game. 

It has the following shape: $[N , N , (M \cdot T + L)]$:

\begin{itemize}
    \item $N$ is the board size ($N = 8$ in chess)
    \item $M$ is the number of different pieces on the board, 
    \begin{itemize}
        \item Two players with six types of pieces each
        \item Every piece is represented by its own 8x8 board of boolean values
        \item For every square: $1$ if the piece is on that square, $0$ if it isn't
        \item $M = 12$ in chess
    \end{itemize}
    \item $T$ is the number of previous moves that are used as input, including the current move. 
    \begin{itemize}
        \item AlphaZero used $T = 8$ for both chess, shogi, and Go.
        \item This gives the network a certain history to learn from
    \end{itemize}
    \item $L$ represents a set of rules specific to the game
    \begin{itemize}
        \item $L = 7$ in chess
        \item $1$ plane to indicate whose turn it is
        \item $1$ for the total number of moves played so far
        \item $4$ for castling legality (both players can castle kingside or queenside under certain conditions)
        \item $1$ to represent a repetition count (in chess, 3 repetitions results in a draw). 
    \end{itemize}
\end{itemize}

$\Rightarrow [8 , 8 , (12 \cdot 8 + 7)]$. 

This means that the input to the neural network is composed of 119 8x8 boards of values.
The $M$ planes that encode the pieces are repeated $T$ times in the input, resulting in 112 boards.
For the planes representing integers, every square in that 8x8 plane will be assigned the integer value.
Other planes are one-hot encoded boolean boards.

\subsubsection{Neural network layers}

DeepMind tested multiple neural network architectures for AlphaGo Zero \cite{NeuralNetworksChessprogramming}. 
The following parts were used in these networks:

\begin{itemize}
    \item A convolutional block, which consists of a convolutional layer, followed by a batch normalization layer, activated by ReLU.
    \item A residual block, which consists of two convolutional blocks and a skip-connection.
\end{itemize}

The skip-connection is used to combine the input of the residual block with the output of the previous residual block.
This is done to avoid the `degradation problem':

\begin{quotation}
    ``When deeper networks are able to start converging, a degradation problem has been exposed: 
    with the network depth increasing, accuracy gets saturated [...] and then degrades rapidly. 
    Unexpectedly, such degradation is not caused by overfitting, 
    and adding more layers to a suitably deep model leads to higher training error'' \cite{heDeepResidualLearning2015}
\end{quotation}

The following networks were tested by DeepMind during development of AlphaGo Zero \cite{MasteringGameZero}:

\begin{itemize}
    \item \textbf{`dual-res':} a single tower of 20 residual blocks with combined policy and value heads. This is the architecture used in AlphaGo Zero.
    \item \textbf{`sep-res':} two towers of 20 residual blocks each: one with the policy head and one with the value head.
    \item \textbf{`dual-conv':} a single tower of 12 convolutional blocks with combined policy and value heads.
    \item \textbf{`sep-conv':} two towers of 12 convolutional blocks each: one with the policy head and one with the value head. This is the network used in AlphaGo.
\end{itemize}

AlphaZero uses the same network architecture as AlphaGo Zero: dual-res.

\subsubsection{Neural network output}

The neural network has two outputs: a policy head, which represents a probability distribution over the possible actions, 
and a value head, which represents the value of the current position.

While the value head simply outputs a single float value between -1 and 1, the policy head is quite a bit more complicated.
It outputs a vector of probabilities, one for each possible action in the chosen game. 
For chess, 73 different types of actions are possible:

\begin{itemize}
    \item 56 possible types of `queen-like' moves: 8 directions to move the piece a distance between 1 and 7 squares.
    \item 8 possible knight moves
    \item 9 special `underpromotion' moves:
    \begin{itemize}
        \item If a pawn is promoted to a queen, it is counted as a queen-like move (see above)
        \item If a pawn is promoted to a rook, bishop, or knight, it is seen as an underpromotion (3 pieces)
        \item 3 ways to promote: pushing the pawn up to the final rank, or diagonally taking a piece and landing on the final rank
        \item $\Rightarrow 3 \cdot 3 = 9$
    \end{itemize}
\end{itemize}

These 73 actions are each represented by a plane of 8x8 float values. Say the first plane is a queen-like move
to move a piece one square northwest, the second plane could be the same type of move, but a distance of two squares, and so on.
The squares on these planes represent the square from which to pick up a piece. 

% TODO: example

The result is a $73 \cdot 8 \cdot 8$ vector of probabilities, so $4672$ float values.  

\subsection{Training the network}

To train this type of network, it's necessary to create a large dataset. 
This is done by letting the engine play against itself for a large number of matches. 
For every action taken by the agent, data is collected and stored in the training set.
For complex games like chess, shogi and Go, this training set needs to be huge
because of the extremely large number of possible situations.

\subsubsection{Tensor Processing Units (TPU)}

Because of the requirement to play a large number of matches against itself, it was necessary to calculate
MCTS simulations in parallel on as fast as possible hardware. 
To help with these calculations, DeepMind used Google's newly created Tensor Processing Units (TPU) \cite{TensorProcessingUnit2022}.
A TPU is an application-specific integrated circuit (ASIC \cite{ApplicationspecificIntegratedCircuit2022}) that is specifically built for machine learning with
neural networks. Since 2018, these TPUs have been made publicly available to rent through Google's Cloud Platform. Smaller TPUs 
can be purchased from Google. 

\subsection{Leela Chess Zero}

Leela Chess Zero (lc0) is a free, open-source project that attempts to replicate the results of AlphaZero \cite{LeelaChessZero2022}. 
Lc0 was adapted from Leela Zero \cite{LeelaZero2021}, a Go computer that attempted to replicate the results AlphaGo Zero \cite{AlphaGoZero2022}. 

It is written in C++ \cite{Lc02022}, and it has managed to play at a level that is comparable to the current best version of StockFish.
Because lc0 is a community driven project, volunteers can help create training games through self-play using their own computers.
This made it possible to feed millions of chess games into the network. 

%%%%% Technical research %%%%%
\newpage
\section{Technical research}

% Volgende zaken worden verwacht:
% Beschrijving van de ontwikkelde software
% Opbouw applicatie: structuur/opbouw van het resultaat
% Achterliggende technologieën
% Overwonnen moeilijkheden/problemen
% …

\subsection{Introduction}

Creating the chess engine required programming the following parts:

\begin{itemize}
    \item The MCTS algorithm
    \item A tree data structure with nodes and edges
    \item The neural network
    \item The training pipeline
    \item The evaluation pipeline
    \item A way to store every move to a dataset
    \item A class to make the engine play against itself
    \item A GUI to play against the engine
    \item Docker containers for easily scaling and distributing the program
\end{itemize}

All code was written in Python. The neural network was made using the TensorFlow Keras library.
Chess rules and helper functions were implemented using the open-source library python-chess \cite{PythonchessChessLibrarya}.

\subsection{Class structure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{img/class-structure.png}
    \caption{Basic class structure for the code responsible for playing a game}
\end{figure}

To play a game, an object of the Game class is created. The game data is stored in an object of the Environment class.
For every player, the Game class creates an Agent that can interact with the environment. Every agent keeps its own 
MCTS tree, and has access to the neural network to send inputs to.

\subsubsection{Making one move}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/ChessRL-schematic.png}
    \caption{Flowchart: making one move}
\end{figure}

To make a move, an Agent (white or black) observes the environment: the chessboard.
The agent calls upon the MCTS class to create a tree with as root the current state of the chessboard.
The MCTS class will run the MCTS algorithm hundreds of times. This amount is configurable in the config file. 
Higher amounts result in a more accurate estimation of the position's value, but also in longer 
computation times.

Every MCTS simulation, the neural network will be called to evaluate a position. The two outputs, 
the policy and the value, will be used to update the tree. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{img/MCTS-choose-move.png}
    \caption{Example tree after 400 simulations. N = number of times the selection step selects that edge}
\end{figure}

After the simulations are done, the agent will pick the best move from the tree. 
It can do this either deterministically, by simply choosing the most visited move, or stochastically,
by creating a uniform distribution of the visit counts and picking a move from that distribution.

Stochastic selection is better when creating a training set, as it will result in a more diverse dataset.
Deterministic selection is better when evaluating with a previous network, or playing against the network competitively. 
In that case, picking the most visited move is the best choice.

\subsection{The neural network}

Initially, a prototype of the neural network was created with randomly initialized weights.
The Python class to create the model was immediately made with customizability in mind: 
the input and output shapes can be given as arguments, and the sizes of the convolution filters 
can be changed using a configuration file.

The neural network architecture is the same as AlphaZero's (see the Research section). 

\subsection{A tree structure with nodes and edges}

As mentioned before, the MCTS algorithm creates a tree structure to represent 
the possible future states after the current position. The tree consists of nodes (chess positions) 
and edges (the moves between positions).

The Node class holds the following data:

\begin{itemize}
    \item The position: a string representation of the board using the Forsyth-Edwards Notation (FEN) \cite{ForsythEdwardsNotation2022}
    \item The current player to move (boolean)
    \item A list of edges connected to this node
    \item The visit count of this node, initialized to 0
    \item The value for this node, initialized to 0
\end{itemize}

The Edge class represents a move. It holds the following data:

\begin{itemize}
    \item The input node (the position from which the move was made)
    \item The output node (the resulting position after taking the move)
    \item The move itself: an object of the Move class from the python-chess library, which holds:
    \begin{itemize}
        \item The source square and the target square of the move
        \item If the move was a promotion: the piece it was promoted to
    \end{itemize}
    \item The prior probability of this move
    \item The visit count of this edge, initialized to 0
    \item The value for this action, initialized to 0
\end{itemize}

The tree can also be plotted using the Graphviz library \cite{Graphviz}. A recursive function was written to 
create the tree and output it to an SVG file. 

\subsection{The MCTS algorithm}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/mcts-alphazero.png}
    \caption{The four steps in AlphaZero's MCTS algorithm \cite{bodensteinAlphaZero2019}}
\end{figure}

A class was written to hold an MCTS tree. It always saves the current position as the root node.

\subsubsection{The selection step}

For the selection step, the following UCB formula \cite{silverMasteringChessShogi2017a} was used to determine which edge to select:

\begin{center}
    \begin{equation}
        UCB = \Big(\log({\frac{(1 + N_{\text{parent}} + C_{\text{base}})}{C_{\text{base}}}}) + C_{\text{init}}\Big) \cdot P \cdot \frac{\sqrt{N_\text{parent}}}{(1 + N)}
    \end{equation}
\end{center}

\begin{itemize} 
    \item $C_{\text{base}}$ and $C_{\text{init}}$ are constants that can be changed in the config file. The same values as AlphaZero were used.
    \item $N_{\text{parent}}$ is the visit count of the input node
    \item $N$ is the visit count of the edge
    \item $P$ is the prior probability of the edge
\end{itemize}

The selection step combines this UCB formula with the edges action-value and visit count:

\begin{center}
    \begin{equation}
        Q = \frac{W}{N+1}
    \end{equation}
\end{center}

\begin{center}
    \begin{equation}
        V = 
        \begin{cases}
            UCB + Q & \text{if white} \\
            UCB - Q & \text{if black}
        \end{cases}
    \end{equation}
\end{center}

The edge with the highest value ($V$) is selected. After selection, the 
visit count for the edge's output node is incremented by one. 
We call this output node the `leaf node'. 

\subsubsection{The expansion step}

The leaf node is expanded by creating a new edge for each possible (legal) move.
If there are no legal moves, the outcome (draw, win, loss) is checked and the leaf node
is passed to the next step in the algorithm.

If there are legal moves, the leaf node is given as an input to the neural network.

While AlphaZero uses an input shape of 119 8x8 boolean boards, I opted for a much lighter 
input shape. The input used in this project only has 19 boards:

\begin{itemize}
    \item 1 board to show which turn it is: 1 is white and 0 is black
    \item 4 boards to show if each player still has castling rights
    \item 1 board to show if a draw is possible after 50 moves without a capture or pawn move
    \item 12 boards to show the positions of the pieces on the board
    \item 1 board to show the en passant square: if a pawn can be taken en passant, this square is set to 1
\end{itemize}

For example: 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{img/input-boards2.png}
    \caption{Board example}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/input-boards.png}
    \caption{The input state converted from the above example board}
\end{figure}

Here, the last square shows the square were en passant is possible.
Grey padding was added to make a better visual presentation of the output planes.

The neural network will return the policy and the value of the position.
As described in the research part of this thesis, the value is a float between -1 and 1, and the 
policy is a 73x8x8 tensor of floats. This policy is then mapped to a dictionary, where keys are moves and 
the values are the probabilities of each move.

The neural network gets no information about the rules of chess, so it is necessary to filter
out the illegal moves. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/output-planes/unfiltered.png}
    \caption{A subset of the 73 8x8 planes from the policy output. The brighter the pixel, the better the move.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/output-planes/filtered.png}
    \caption{The same subset, but with the illegal moves filtered out}
\end{figure}

The above two images were made using a trained model. The policy output of an untrained model with random weights 
looks like this:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{img/output-planes/random-model-unfiltered.png}
    \caption{Some output planes from an untrained model.}
\end{figure}

(Black padding was used instead of gray to make it clearer.)

This clearly shows the trained model has at least some level of understanding of which moves are legal, 
without having been given any knowledge about the rules of chess. The model isn't even told we're playing chess: it
learns the correct outputs completely on its own. 

\subsubsection{The evaluation step}

The value received from the neural network is now assigned to the leaf node.

\subsubsection{The backpropagation step}

The value from the leaf node is now also added to every selected node in the path from the root to the leaf node.
Concretely, for every edge in the traversed path, the following values are changed:

\begin{itemize}
    \item The edge's input node's visit count is incremented by 1
    \item The edge's visit count is incremented by 1
    \item The edge's value is incremented by the value of the leaf node
\end{itemize}


\subsection{Creating the dataset}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{img/trainingset.png}
    \caption{The training set consists of: the input state, the move probabilities, and the eventual winner}
\end{figure}

To improve the performance of the neural network, the algorithm needs to play against itself for a large number of games.
Every move is saved to memory in a simple Python list. Once the game is over, the winner is assigned to every move in memory.
The whole game is then saved to a binary file in NumPy's .npy format. 
When a new game starts, the memory is cleared.

\subsubsection{Creating a dataset from puzzles}

Because creating data is extremely time-consuming, I came up with the idea to also
create data from chess puzzles. A chess puzzle is a position with one simple goal: find the best move or sequence of moves.
Lichess.org, the open source chess website, has a huge database of over 2 million of these puzzles publicly available for free \cite{LichessOrgOpen}. 
These are the most common puzzle categories:

\begin{itemize}
    \item Mate-in-X: find the best moves to checkmate the opponent in a maximum of X moves
    \item Capturing one of the opponent's undefended pieces
    \item Getting a positional advantage
\end{itemize}

The idea is to let the agent play from a certain given position (the start of the puzzle), 
instead of from the start of the game, and see if the agent can find the correct solution.
To stay true to the idea of creating a chess engine without any human intervention,
the agent is never told if it has reached the end of the puzzle. It simply plays moves
until a checkmate happens. A move limit is imposed to prevent the agent from taking too long, 
and if this limit is reached the puzzle is discarded. 

This would in theory give the model a better understanding of common chess tactics, without having to play
whole games from start to finish. 

Apart from playing full games, the network was trained with mate-in-1 and mate-in-2 puzzles. 
This is because these kinds of puzzles end quickly (and are thus less time-consuming), 
and they have a clear winner: the player who can checkmate the opponent.
The Lichess database has over 400,000 puzzles of these two types. 

One problem I noticed with this approach, especially with mate-in-1 puzzles, is that 
the network will learn that whoever is playing the first move will always win. 
The value network will always assign a value close to 1 when it's white's turn, 
and a value close to -1 when it's black's turn.
That's why it is crucial to maintain a good balance between real games and puzzles.

\subsection{Training the neural network}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/training.png}
    \caption{The training pipeline}
\end{figure}

To train the neural network, every saved game's binary file is loaded 
into memory. The memory is shuffled to avoid the neural network accidentally learning
time dependent patterns. Random batches of the training set are then processed through the
neural network.

The position is used as the input to the neural network. The network's outputs are then 
compared to the move probabilities and the winner from the dataset:

\begin{itemize}
    \item The move probabilities are converted to a 73x8x8 tensor, which is compared with the output of the network.
    \item The winner is compared with the value output of the network.
\end{itemize}

The training pipeline employs two separate loss functions. The policy head uses categorical cross-entropy
and the value head uses mean squared error.

\subsubsection{The first training session}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{img/plots/first-training.png}
    \caption{First training session}
\end{figure}

Naturally, the first training session started with a random model. 
The dataset was created by running self-play for many hours, resulting in a dataset of around
76,000 positions. Training was performed with a learning rate of 0.002 with the Adam optimizer, 
and a batch size of 64.

After training, the model was saved to disk, so it can be used to run self-play again.

\subsubsection{The second training session}

The previous model was used as the starting point for the second training session.
A new dataset was made using only games created by that model. 
Due to time constraints, this dataset was only made up of 50,000 positions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{img/plots/second-training.png}
    \caption{Second training session}
\end{figure}

It's clear the model manages to learn something at the start of the training session,
but seems to plateau after 150 batches. Because the first training session started from 
a random model, it managed to learn a lot more before reaching a plateau after around 600 batches.

\subsubsection{Subsequent training sessions}

In further training sessions, some improvements were made to make it easier for the model to learn.
Previously, when the game ended in a draw, the data was assigned a label of 0. 
Because playing semi-random moves often doesn't end in a win for either player 
(partly due to the relatively low move limit), this caused the dataset to mainly consist of drawn games. 
DeepMind could use a much higher move limit, because they had the hardware to play moves a lot faster.

To mitigate this problem as much as possible without having to raise the move limit, 
the data was not always assigned 0 when a draw occurred. Instead, if the game reached the move limit
without a decided winner, the winner was estimated based on the pieces on the board in the last position. 
Every piece gets a value based on its type, with stronger pieces getting higher values:

\begin{itemize}
    \item Pawns: 1
    \item Knights \& Bishops: 3
    \item Rooks: 5
    \item Queen: 9
\end{itemize}

If the difference in scores between the two players is 5 or greater, the player with the highest 
score was assigned the winner. Games where white has a big advantage at the end of the game are 
given a score of 0.25, while games where black has a big advantage are given a score of -0.25.
This fixes the problem of having an unbalanced dataset of mostly draws.
Without this improvement, the model might overfit, resulting in the value output always being 0.

Here are some of the loss graphs of subsequent training sessions:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{img/plots/loss-2022-04-14_20:31:55.png}
    \includegraphics[width=0.45\textwidth]{img/plots/loss-2022-04-17_20:06:09.png}
    \caption{Subsequent training sessions.}
\end{figure}

As is apparent in the graphs, the loss does not decrease much. 
Different learning rates were tested, but that did not help, as there is simply a lack of data to train on.


\subsection{Multiprocessing}

Because self-play is very time-consuming, there needed to be a way to play multiple games in parallel 
on the same system.

\subsubsection{Without multiprocessing}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{img/without-multiprocessing.png}
    \caption{Self-play without multiprocessing}
\end{figure}

Previously, every game was played in its own process, but every agent needed to send predictions to a neural network.
This resulted in every process creating its own copy of the neural network. This is extremely heavy for the GPU, and
it's impossible to scale. Due to VRAM limits, only two games could be played in parallel on an RTX 3070.

\subsubsection{With multiprocessing}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{img/with-multiprocessing.png}
    \caption{Self-play with multiprocessing}
\end{figure}

To solve the issue of parallel self-play, I created a client-server architecture with Python sockets.
The server side has access to the neural network. For every client (the self-play process), 
the server creates a ClientHandler in a new thread.

Here's how one client works:

\begin{enumerate}
    \item The MCTS algorithm runs hundreds of simulations every move. Every MCTS simulation, an input is sent to the network.
    \item The Agent sends the chess position through a socket.
    \item The ClientHandler receives the chess position and sends it to the server.
    \item The Server calls the network's predict function and returns the outputs to the client handler.
    \item The ClientHandler sends the outputs back to the client.
\end{enumerate}

This socket communication does have a small overhead in both time and CPU usage, but it's much faster than the previous method
because it is much more scalable. Here's a comparison of the two methods, with the server running on the first system:

\begin{table}[h!]
    \begin{tabularx}{\textwidth}{@{}Lll@{}}
    \toprule
    & \textbf{No multiprocessing} & \textbf{Multiprocessing (8 games)} \\ 
    \midrule 
    \textbf{RTX 3070 + Ryzen 7 5800H} & 50 simulations/sec & 30 simulations/sec per game \\ \addlinespace
    \textbf{GTX 1050 + i7 7700HQ} & 30 simulations/sec & 15 simulations/sec per game \\ \addlinespace
    
    \bottomrule
    \end{tabularx} 
    \caption{Comparison of non-multiprocessed and multiprocessed self-play}
\end{table}

With eight games in parallel on the first system, I managed to get an average speed of $30 \cdot 8 = 240$ simulations per second.
The second system is much less powerful and needed to connect over Wi-Fi, but it still managed an average of $120$ simulations per second. 

\subsection{A GUI to play against the engine}

The GUI was based on a GitHub project I found that was created for simply visualizing a chessboard with the PyGame library \cite{adefokunChessboardAhirajustice2022, pygame}.
I greatly improved and expanded that project to include a way for the player to interact with the board, play against an engine,
play against yourself, and more \cite{zjefferChessboardZjeffer2022}. I created a pull request to include my changes in the original author's GitHub project,
but it was declined due to being too extensive and out-of-scope for the project \cite{adefokunChessboard2022pull}.

Using this code, a GUI class was created for playing against the engine. The player can choose a color and play against
the engine. Clicking on a piece will highlight its square, clicking on another square will move the piece to that square.
Right-clicking will cancel the move. Not much time was spent on the visual aspects of the GUI, as this was out of scope
for the research project. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{img/chessboard-gui.png}
    \caption{Chessboard example when playing against the engine}
\end{figure}

Promoting a pawn can be done by simply moving a pawn to the last rank. A menu will pop up 
to choose the piece to promote to.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{img/chessboard-promotion.png}
    \caption{Promoting a pawn}
\end{figure}

\subsection{Docker images}

Because the amount of data needed to train the neural network is very large, two docker images were created to make deploying and scaling easier:

\begin{itemize}
    \item A server image that runs the neural network and listens for clients.
    \item A client image that runs self-play and sends chess positions to the server.
\end{itemize}

Using a docker-compose file, the server and client containers can be managed easily. The number of clients 
to run in parallel can be configured in that file, along with many other settings.

It's recommended to run the server on a fast GPU-equipped system, and the clients on a system with many high-performance CPU cores.
The clients do not need a GPU to run self-play.

\subsection{The final project}

This section walks through all executable programs included in the final project \cite{zjefferChessEngineDeep2022}.

\subsubsection{Creating your own untrained AI model}

\begin{minted}{text}
$ python rlmodelbuilder.py --help
usage: rlmodelbuilder.py [-h] [--model-folder MODEL_FOLDER] [--model-name MODEL_NAME]

Create the neural network for chess

options:
  -h, --help            show this help message and exit
  --model-folder MODEL_FOLDER
                        Folder to save the model
  --model-name MODEL_NAME
                        Name of the model (without extension)
\end{minted}

This will create a new model with the name <MODEL\_NAME> in the folder <MODEL\_FOLDER>.
The model parameters (number of hidden layers, input and output shapes if you want to use the network for a different game, the number of convolution filters, etc.)
can be changed by editing the config.py file.

\subsubsection{Creating a training set through self-play}

Creating the docker containers:

\begin{minted}{shell}
# in the repo's code/ folder:
docker-compose up --build
\end{minted}

This will create one server and the number of clients that is configured in the docker-compose.yml file.
That file can also be used as a reference to deploy a Kubernetes cluster for parallel self-play with 
high scalability and reliability. 

Using the created GUI, it is possible to visualize the self-play of all these boards in real-time.
This can be done by setting the `SELFPLAY\_SHOW\_BOARD' environment variable in docker-compose.yml to `true'.
For every replica of the client, a new PyGame window will open where the board will change in real-time. 

You can also manually run self-play or create data using puzzles:

\begin{minted}{text}
$ python selfplay.py --help
usage: selfplay.py [-h] [--type {selfplay,puzzles}] [--puzzle-file PUZZLE_FILE] 
    [--puzzle-type PUZZLE_TYPE] [--local-predictions]

Run self-play or puzzle solver

options:
    -h, --help            show this help message and exit
    --type {selfplay,puzzles}
                        selfplay or puzzles
    --puzzle-file PUZZLE_FILE
                        File to load puzzles from (csv)
    --puzzle-type PUZZLE_TYPE
                        Type of puzzles to solve. Make sure to set a 
                        puzzle move limit in config.py if necessary
    --local-predictions   Use local predictions instead of the server
\end{minted}

\subsubsection{Evaluating two models}

To determine whether your new model is better than the previous best, you can use the evaluate.py script.
It will simulate matches between the two models and record the wins, draws and losses.

In chess, white inherently has a slightly higher chance of winning because they can play the first move \cite{FirstmoveAdvantageChess2022}.
Therefore, to evaluate two models, each model will both play white and black an equal number of times.

\begin{minted}{text}
$ python evaluate.py --help
usage: evaluate.py [-h] model_1 model_2 nr_games

Evaluate two models

positional arguments:
  model_1     Path to model 1
  model_2     Path to model 2
  nr_games    Number of games to play (x2: every model plays both white and black)

options:
  -h, --help  show this help message and exit
\end{minted}

\subsubsection{Playing against the AI}

\begin{minted}{text}
$ python main.py --help
usage: main.py [-h] [--player {white,black}] [--local-predictions] [--model MODEL]

options:
  -h, --help            show this help message and exit
  --player {white,black}
                        Whether to play as white or black. No argument means random.
  --local-predictions   Use local predictions instead of the server
  --model MODEL         For local predictions: specify the path to the model to use.
\end{minted}

This will start the GUI application to allow you to play against the engine. 

\subsection{Porting to C++}

Optimization is extremely important for chess engines, especially when the engine needs to play
against itself to create a dataset. That is why Python was not an ideal choice to implement this chess engine.

Currently, I'm completely rewriting the engine in C++ in my spare time \cite{zjefferChessdeeprlcpp2022}. 
Instead of TensorFlow Keras, I've opted to use PyTorch instead. I chose this framework because 
it's a lot faster than Keras, and to expand my experience with AI frameworks.
So far, I've noticed that the C++ version with PyTorch manages to run the MCTS algorithm 
more than four times faster than the Python version: 200+ simulations per second instead of 50.




%%%%% Reflection %%%%%
\newpage
\section{Reflection}

\subsection{Introduction}

This section is a reflection on the project. It will answer the following questions:

\begin{itemize}
    \item What are the strengths and weaknesses of this research project?
    \item Is the result of the project usable for corporations?
    \item What are the possible obstacles for companies that wish to implement this?
    \item What is the added value for companies?
    \item Which alternatives are there?
    \item Is there a socio-economic impact present?
    \item Is there opportunity for further research?
\end{itemize}

To help answer these questions, I have consulted with Dr. Thomas Moerland, a postdoctoral 
researcher at the Reinforcement Learning Group at Leiden University, in The Netherlands \cite{ThomasMoerlandPostdoc}. 
With his experience in researching AI and specifically Reinforcement Learning, he was able 
to give me some insights into how AlphaZero could impact society in industries besides gaming.
He is also very familiar with AlphaZero, as he implemented a very simple version of it for single player games \cite{blogSinglePlayerAlphaZeroa}.

I have also asked these questions to the following online communities:

\begin{itemize}
    \item The AI Stack Exchange community \cite{zjefferHowCanAlphaZero2022,zjefferWhatCanWe2022}
    \item The Leela Chess Zero Discord community \cite{discordLC0Discord}
    \item The TalkChess forum, a forum affiliated with The Computer Chess Club (CCC) \cite{BachelorThesisYour}
\end{itemize}

\subsection{Strengths and weaknesses}

This project successfully implements the MCTS algorithm as modified by DeepMind for AlphaZero and their preceding work,
AlphaGo and AlphaGo Zero. The resulting chess engine can be played against using the GUI, and it can be used to create
a dataset for further improving the neural network. The project serves as an example of how AlphaZero's MCTS algorithm 
can be implemented in a chess engine using Python and consumer-grade hardware. 

The project offers a way to easily create your own neural network with the rlmodelbuilder.py script.
The code is written with modularity and adaptability in mind: by changing parameters in the config.py file, 
you can easily create a different model for chess, or for similar applications with different input and output shapes. 
The MCTS algorithm can be used as a reference if you want to implement it other applications.

The ultimate goal of this research project was to create a chess engine that is at least capable of winning
against amateur players of around 1000 ELO. This was not possible with my consumer-grade hardware: 
the chess engine still seems to play very randomly and has not found a good winning strategy. 
The biggest hurdle was the lack of computational resources to create a large dataset.

However, the project does offer a solution to create more data through self-play: with the docker containers, 
it is easy for other people to help create a dataset by deploying their own cluster of servers and clients. 
This way, a global dataset could be made by combining self-play data from volunteers around the world. 
It can then be used to train a new model. 

This is exactly how Leela Chess Zero operates: the developers created a simple program for volunteers to run,
which makes the latest model play against itself and uploads the data back to the developers. Once there is
enough data, the developers can retrain the model. By continuously repeating this process, Leela Chess Zero
slowly became a powerful chess engine on par with the best chess engines in the world \cite{TopChessEngine2022}.

\subsection{Is the result of the project usable in the corporate world?}

While AlphaZero and its variants have mostly been used in gaming applications, 
the project is not limited to this. For instance, AlphaGo has been used by Google
to properly cool their data centers more efficiently. The algorithm helped lower
data center cooling costs by 40\% \cite{decemberHasGoogleCracked2020}. 
It does this by periodically pulling data from many sensors and feeding that data to 
a deep neural network to predict how different combinations of potential actions
will affect future energy consumption \cite{HowAIHelps2018}.

Theoretically, this project can be used to solve virtually any problem that can be
defined as an agent acting upon an environment with discrete state-action spaces \cite{ReinforcementLearning2022a}, \cite{nbroAnswerAreThere2020}.

Reinforcement learning in general has been used to solve a wide range of problems.
Many websites that have recommendation systems have already used reinforcement learning 
to improve which products get recommended to users \cite{nbroAnswerAreThere2020}. 
For example, Netflix's homepage is made up of thumbnails of movies and TV shows, which are chosen 
by contextual bandits in order to maximize click rate \cite{blogArtworkPersonalizationNetflix2017,surmenokContextualBanditsReinforcement2017}.
This avoids the need for waiting to collect data from users, training a model,
waiting for an A/B test to conclude, to then finally recommend the best thumbnail.
By then, too many users have ignored an item they would have otherwise clicked on.

Another example in the gaming industry is the use of multiagent reinforcement learning 
to play as a single team of five agents in the game of \textit{DotA 2}. \textit{DotA 2} is a multiplayer videogame where 
two teams fight against each other, so team play is crucial. The goal of the game is to destroy the 
enemy team's base. This is a very difficult task for an AI to accomplish, as completing this goal often takes 
around 40 minutes. Simple reward functions are therefore impossible. OpenAI has successfully created 
an AI that can defeat even the best DotA players in the world with large scale deep reinforcement 
learning \cite{openaiDotaLargeScale2019,OpenAIFive2019}.

In the field of robotics, (deep) reinforcement learning can be used to teach a robot
through self-play to perform simple tasks that previously required manual programming.
Some examples of this are a robot learning how to flip pancakes \cite{petarkormushevRobotLearnsFlip2010}
and a robot learning how to play table tennis \cite{mpikybrollLearningRobotTable2012}.

\subsection{Possible obstacles for companies that wish to implement this}

Depending on the application, the amount of data necessary to train a sufficiently powerful model
can be extremely large. It's necessary for the company to have a good infrastructure to 
create this data. Companies can also invest in cloud solutions like the Google Cloud Platform
to take advantage of their TPUs \cite{CloudComputingServices}.

Because these types of algorithms need to be trained through self-play, applications should be written
in a programming language that is capable of training and inferring neural networks, 
but should also be very performant and efficient. 
Fast, low-level programming languages like C++ often require much more development time
to create the same application compared to high-level programming languages like Python. 
This should also be factored into the development process.

AI frameworks like TensorFlow and PyTorch are available in both Python and C++, 
but documentation and support is very limited in the C++ versions. This means companies 
who wish to implement this type of application will need to hire C++ developers who
are also experienced with neural networks.

\subsection{The added value for companies}

Options are always useful, and this project offers a way to solve problems 
using AI without having to manually collect data. The reinforcement learning agent 
will collect data for you by playing against itself. This means the algorithm will
get better in a `fire-and-forget' manner: just launch a cluster of self-play agents
and wait until it gathers enough data. You could create an automatic pipeline to
retrain the model after a set amount of time or after a certain amount of games.

\subsection{Alternatives}

For chess specifically, there are many chess engines that can be used for both analysis
and to play against. StockFish is the most popular, and as mentioned before, 
Leela Chess Zero is also open source and works the same way as AlphaZero. 

For industries other than the gaming industry, many problems are solvable by using
machine learning algorithms instead of reinforcement learning solutions. 
Supervised learning methods have a very wide range of applications, provided 
you have the data to train a model.

However, if you don't have a means to gather data, there are many reinforcement
learning algorithms other than AlphaZero:

\begin{itemize}
    \item Q-learning
    \item SARSA
    \item DQN
    \item Actor-Critic methods like A2C or A3C
    \item \dots
\end{itemize}

Currently, sending inputs from the client to the server for inference
happens through Python sockets. A good alternative for that is gRPC \cite{IntroductionGRPC}. 
gRPC would allow the clients to directly call a method on the server without having to 
resort to Python sockets, which might be slower. 

\subsection{Is there a socio-economic impact present?}

Sadly, algorithms like AlphaZero fail at solving even relatively simple problems like chess when the model 
does not have enough data to train on.  \cite{ThomasMoerlandPostdoc}. 
Given enough time and data, though, these types of algorithms can be used to achieve superhuman
performance in applications that were previously impossible to find optimal solutions for.

DeepMind's ultimate goal is to ``[...] solve intelligence, developing more general and capable 
problem-solving systems, known as artificial general intelligence (AGI).'' \cite{DeepMind}.
This is a slow and extremely difficult process, but DeepMind is confident AlphaZero and 
its variants are a big step towards that goal \cite{AlphaZeroSheddingNew}.

Others are more pessimistic about the progress DeepMind has made \cite{moerlandEmailExchangeTuur22, dukezhouAnswerAlphaZeroExample2018},
explaining that while AlphaGo (Zero) and AlphaZero have certainly been a huge breakthrough in their respective applications, 
this does not necessarily mean their results are a particularly big step towards AGI. 

Dr. Thomas Moerland had this to say:

\begin{quotation}
    ``I think there is a huge gap between the simple games we now solve and the 
    `artificial general intelligence' that DeepMind is trying to achieve.
    I believe that most progress won't come from innovations in algorithms, but from innovations in computation.
    For general AI, we need incredibly strong simulators where a huge amount of data
    can be passed through for a very long time. 
    Just like the millions of years of evolution our brains have been through, our brains
    have been gathering data non-stop throughout our lives.'' \cite{moerlandEmailExchangeTuur22}
\end{quotation}


\subsection{Is there opportunity for further research?}

While reinforcement learning has existed for a long time, deep reinforcement learning techniques 
have only recently been used. This is because of the lack of computational resources to train
a well-performing model. With high performance computing devices and cloud solutions getting cheaper
and more powerful, a lot of research opportunities open up.
Deep Reinforcement learning for autonomous driving has recently become an active area of research 
in both academia and industry \cite{DeepReinforcementLearning2022a}.

For this project specifically, there are still lots of research opportunities to try to solve the problem
of creating a large enough dataset to train a sufficiently powerful model. To me, the most promising solution is to
pretrain the model using real data, as there are millions of chess games available on the internet for free.
However, other avenues should definitely be explored to avoid having to collect that much data in the first place.

%%%%% Advice %%%%%
\newpage
\section{Advice}

\subsection{Introduction}

This section gives advice to people who wish to implement something similar to the project.
It includes recommendations and tips for when people want to program an application that uses
the algorithms and concepts researched in this thesis.

\subsection{When should you use this technology?}

This algorithm has a very specific purpose. It should only be used in situations where
both the action space and state space are discretely defined. Both of these should also
be relatively small, as bigger action spaces and state spaces will require much more 
data to successfully train a model. 
Therefore, it is important to keep the problem as simple as possible. 

\subsection{Recommendations}

When you want to implement AlphaZero or similar algorithms, it is recommended to
prioritize writing code in the most efficient way possible. Every step of the algorithm
should be programmed with multiprocessing and multithreading in mind. Furthermore,
the choice of programming language should be considered carefully. While most AI 
frameworks are primarily written for use in Python, and most documentation is written
for Python, the language just isn't fast enough for something that needs this much data through self-play. 
C++ would be a better choice.

As was apparent from the results of the research project, generating training data 
takes a very long time. A good solution to this problem is to first pretrain the model
on a small dataset. This will result in a model that is at least slightly familiar 
with the environment. Without a pretrained model, the algorithm will start from nothing
and make completely random actions. With a pretrained model, the algorithm will have some 
idea on what decisions to make based on the data it was given in pretraining.

% TODO

\subsection{Tips when programming a similar application}

This section will give some useful tips for programming an application that uses
AlphaZero's MCTS algorithm. 

\subsubsection{Creating the tree data structure}

The MCTS algorithm requires a tree to be constructed. As mentioned before, the nodes of the tree
are the positions of the game, and the edges between those nodes are the actions that can be taken
from those positions. 

It is important to avoid memory management issues when creating the tree. The MCTS algorithm 
can create quite large trees, especially when using hundreds of simulations. Every node object
should therefore only include the necessary data. For example, a very early version of my MCTS algorithm 
included all previous moves of the game in every node, creating a huge memory leak. This was fixed
by only storing the current board state as a FEN string in each node.

\subsubsection{Programming the MCTS algorithm}

When programming the MCTS algorithm, it is crucial to make every step as efficient as possible.
In the expansion step of the algorithm, the state of the leaf node needs to be converted to an object
that can be used as an input in the neural network. The function that converts the leaf's state 
can be very slow if it's not programmed efficiently. As that function needs to be called for every
simulation, every line of code should be optimized for speed.

For example, in an early version of my algorithm, this line was used to create an array of zeroes of 
shape 73x8x8:

\begin{minted}{python}
arr = np.asarray([0 for _ in range(64*73)]).reshape(73, 8, 8)
\end{minted}

This was later improved to:

\begin{minted}{python}
arr = np.zeros(64*73).reshape(73, 8, 8)
\end{minted}

Running these two lines repeatedly for a thousand times each took 255ms for the first version, and 1ms for the second version.
This is why it is important to regularly test the speed of the code: sometimes you might write something
that can be greatly optimized. 


Calling the neural network is definitely the slowest step of the algorithm. A good way to take 
advantage of this is to send batches of data to the network instead of one input at a time. 
This can be done by creating multiple threads to `crawl' through the tree at the same time 
during the selection step of the algorithm. 
Once every thread has finished by finding a leaf node, every leaf node can be sent to the network at once. 
For every output received by the neural network, a thread can again be started for the third and fourth steps. 
This will result in a significant speedup. 
The difficulty of programming this is mainly making sure the `crawlers' during the selection step
don't all select the same nodes. That would result in a less explored MCTS tree.

\subsubsection{Choosing the right input for the neural network}

As mentioned in the technical research part of this thesis, I opted for a smaller input
shape than the one defined in AlphaZero's paper. This is because I wanted to make the 
neural network as fast as possible. 

You should carefully consider what data the neural network would need to make a decision, 
based on the problem you're trying to solve. For example: when Google used AlphaGo to make
their data center's temperature control more efficient, they opted to use the sensors in the
data center as inputs to the neural network. 
You should make sure to omit any unnecessary data from the input. Is every input feature important?
If you need more features, can you apply feature expansion with the inputs you already have?

\subsubsection{Writing tests}

With complex applications like chess engines, it is crucial to write tests to make sure
every function is working as intended. One small mistake might make it impossible for the 
neural network to learn from the data, and these bugs might be hard to find.

For instance, I wrote a function that converts the inputs and outputs of the neural network
to black-and-white images. Every pixel in those images are converted from 0-1 to a grayscale value of 0-255.
If there are mistakes in your code, viewing these images will help you find them.

\subsubsection{Restricting the search depth}

In this project, a limit was set on the amount of simulations the MCTS algorithm can perform.
When setting this limit at 400, the depth of the search tree was always around 5. This is quite low,
but that is because the branching factor of chess is so high. There are simply too many possible actions
to pick in every state to be able to reach a high depth.  

If the branching factor in your application is lower, this would result in a deeper search tree.
This might not be necessary for your use-case, so limiting the depth of the search tree might be a good idea
in order to maximize the speed of the algorithm. Setting a time limit is also a viable strategy 
to make sure time isn't wasted selecting actions that aren't viable in the first place \cite{masonAnswerHowWe2019}.

\subsubsection{Increasing exploration}

To make sure the algorithm doesn't always pick the same actions, adding more randomness is a good idea. 
AlphaZero did this by adding Dirichlet noise to the prior probabilities in the root node.
This ensures that every possible move in the current position is tried at least once. The MCTS search 
will then overrule the bad moves, but due to the stochastic selection of moves, 
it will have a small chance to select them anyway \cite{monkPurposeDirichletNoise2018,DirichletDistributionWikipedia}.

\subsection{Step-by-step plan}

This section broadly explains step-by-step how to program similar applications other than chess.

\subsubsection{Step 1: The neural network}

Try to figure out what the input and outputs need to be. The input of the network should contain
only the information necessary to make an accurate decision. One of the outputs should describe 
the `value' of the input, while the other should give a win probability for each possible action in the given state.

Depending on the problem, the number of layers could be very high. It is recommended to use residual blocks
for the hidden layers, because the convolutional filters are suitable for a very wide range of applications, 
and the skip-connections solve the degradation problem as described in the research section.

\subsubsection{Step 2: The MCTS algorithm}

Now that the neural network is set up, you can use it in the MCTS algorithm.
Because this is quite a complex algorithm, I recommend you to spend a lot of time brainstorming 
how to implement it as efficiently as possible. Try to make sure you completely understand the algorithm
before implementing it.

Start by defining what the nodes and edges in the tree should contain. Object-oriented programming is recommended,
so create a class for each of these objects. 

Program every step of the MCTS algorithm in its own function, and then try to execute the algorithm on 
one position.

\subsubsection{Step 3: Self-play}

After the MCTS algorithm can successfully be executed on one position, you can start implementing it 
in a self-play environment. For chess, many of these environments are already available. For your application,
it is advisable to write your own environment as efficiently as possible. Visualizing self-play is nice to have,
but unnecessary, as it can often result in a slower algorithm.

\subsubsection{Step 4: Saving actions}

When self-play works, the next step is to save every action the input makes. This will be your dataset. 
It consists of the input state of the network, the value that the MCTS algorithm assigned to the root node, 
and the distribution of actions it selected from the root node.

Here, it's crucial to save the actions in a way that is both easy to read when loading the dataset to train,
and small enough to save disk space. For chess, I only saved the FEN string of the position, the value, and a dictionary
of moves and their probabilities.

Execute self-play to create a dataset as large as possible. Depending on your application and your hardware, 
you might need to do this for multiple days. 

\subsubsection{Step 5: Training the neural network}

Now that your dataset is created, it is time to train the neural network. 
Start by defining the loss functions for the outputs. I used the cross-entropy loss function for the probabilities,
and mean squared error for the value output.

Train your model using the dataset that was created with it. Load the data in batches, but make sure to 
randomly shuffle the data before training. Also don't forget to create graphs of the loss functions during training.

When your model is trained, save it and use it to create a new dataset through self-play. 
Retrain your model from the new data. Don't reuse data from previous models.

%%%%% Conclusion %%%%%
\newpage
\section{Conclusion}

This thesis was created to critically evaluate my research project that tried to 
answer the question \textbf{`How to create a chess engine using deep reinforcement learning'}.
The aim of the project was to create a chess engine based on DeepMind's AlphaZero in
Python with TensorFlow Keras. This was attempted by programming the MCTS algorithm 
and a neural network that can train on data collected from self-play. 
The ultimate goal of the project was not to create a chess engine that was capable of 
beating AlphaZero, but rather to create a simple version of AlphaZero that could 
beat amateur chess players of around 1000 ELO. 

Sadly, the resulting chess engine has not managed to reach this goal.
This is because of a lack of computational resources necessary to create a big enough
dataset through self-play. However, this thesis does offer some solutions to this problem:

\begin{itemize}
    \item Pretrain the model on a small dataset with supervised learning
    \item Use docker containers to set up a cluster of self-play agents
\end{itemize}

This thesis also serves as a warning to people who wish to implement similar algorithms
without having the necessary access to powerful hardware. It suggests investigating certain 
alternatives to AlphaZero, such as other reinforcement techniques like Q-learning, SARSA, and DQN,
but also supervised learning techniques if training data is available.




%%%%% Bibliography %%%%%
\newpage
\section{Bibliography}
\renewcommand{\bibname}{}
\printbibliography[heading=none]

%%%%% Appendix %%%%%
\newpage
\section{Appendix}

\subsection{Report guest speaker: ML6}

\subsubsection{Introduction}

On the 20th of January, I attended a very interesting lecture by Matthias Feys, 
CTO of ML6. ML6 is an AI consultancy company based in Belgium, Germany, The Netherlands 
and Switzerland that serves customers across Europe. They have over ninety 
machine learning \& data engineering experts and are the largest and fastest 
growing AI company in Europe. 

He talked about how the company tries to increase the explainability of their AI 
solutions. 

\subsubsection{What is Explainable AI?}

The inner workings of how an AI gets to a certain decision is often very difficult to explain.
Say you have some inputs and a neural network that takes those inputs and produces
one output: Yes or No. Without explainability, you can't tell the customer why the 
network makes the decision. The customer just has to take your word for it.

An improvement to the above example is instead of outputting a boolean value, 
a percentage could be used instead. This would allow the customer to understand
how sure the network is of its decision.

While AI technology is improving extremely quickly, adoption of these technologies
by other companies is often still a hurdle to overcome. Here are three scenarios in 
which Explainable AI can drive adoption. All of these scenarios have the goal of
building trust in the AI.

\begin{enumerate}
    \item AI is weaker than human
    \item AI is on par with human
    \item AI is stronger than human
\end{enumerate}

\textbf{AI is weaker}

In this scenario, adoption is very difficult, which is why explainability
will be primarily about improving the AI. By explaining how the AI makes a decision,
the customer can understand how they can help improve it. 

You can visualize the outputs of a model. For example: drawing lines and bounding 
boxes around the output of a neural network is a great way to show what the 
contribution of the model is.

You can improve the model by capturing a lot of data and either letting the customer label
the data themselves or fixing incorrect labels. This is called Active Learning, and it
focuses on the data where the model is weak. 
While waiting for a model to improve, you can manually create a simpler model that can
solve the specific edge case that went wrong before.

\textbf{AI is on par}

When the AI is as good as the human counterpart, you want to build trust in the
AI by discussing how it works. It's difficult to convince someone to consider 
using the AI over the human solution if they are equally valid options. 

In this scenario, explainability can help humans to use the result of the AI 
to their advantage. You're basically letting the AI and the human work together
to solve a problem.


\textbf{AI is stronger}

Stronger than human performance is great, but it's better if you can also explain
why it's the better option. Explainability here is about explaining a complicated
concept and informing humans.

The information you can learn from an AI is incredibly useful to companies in order
to improve their machines, processes, and products. 

\subsubsection{Five generic design patterns}

Here are 5 tips that are useful for explaining AI.

\begin{enumerate}
    \item Problem reframing
    \item Interpretable models
    \item Feature attribution
    \item Transparency \& transferability
    \item Intuitive visualizations
\end{enumerate}

\textbf{Problem reframing}

It's often better to completely reframe the problem before you start to think about
programming an AI to solve it. For example: when a customer asks to classify whether
a car is damaged or not, you can reframe the problem as an object detection problem.
An object detector can then be trained to detect specific locations of damage on the car.
This still answers the customer's initial needs, but is much easier to explain.

\textbf{Interpretable models}

Some models are generally more interpretable than others. Linear models, for example,
can be very easy to interpret, but these aren't very performant. LSTMs on the other hand,
can be very performant, but they are hard to interpret. It's crucial to find a correct
balance between both performance and interpretability. AI architectures can be combined
to solve a problem in a way that is both performant and easy to understand.

\textbf{Feature attribution}

It's important to understand which features are contributing the most or the least 
to the output of the model. For example: if you have a model that recommends a product,
you can show which features were had the most impact when recommending that specific product.
This can be done with techniques like SHAP values, saliency maps or LIME.

\textbf{Transparency \& transferability}

If the data changes drastically, for example due to a crisis like the coronavirus, the model might not be relevant anymore.
If a model uses another model, it is important that the first model is explainable as well. 
Validation of data is also very important: if the data is filled with mistakes, 
the output can not be trusted. That is why it's necessary to run a standard set of checks
on the data to uphold the quality of the dataset.

\textbf{Intuitive visualizations}

For example, when a model recommends a product, a visual representation can be shown of 
other products that are similar to the recommended product, from most similar to least similar.
This gives the customer a good sense of how it ranks products to your preferences.

For computer vision, saliency maps can show the importance of each pixel in the image when 
making a decision.

\subsubsection{Conclusion}

The main conclusion of the talk was that Explainable AI is not just about using SHAP to explain
the features, but it is about much more than that.
Explainability is not just part of developing a model, 
it is present in the whole pipeline from start to finish: it starts with the framing of the problem
and ends with the user interface and user experience.

The goal is to increase trust and adoption.

\subsubsection{Critical reflection}

I thought it was a very interesting talk about a problem that isn't always given adequate 
attention in the AI community. It is also something I am personally guilty of: when developing
an AI, explainability is one of the last things I think of, when it should be part of the 
developing process from the start. For example, my bachelor thesis about creating a chess engine using deep reinforcement learning
is a project where almost no explainability is possible. This makes developing the AI 
very difficult, as it's not clear why the model makes its decisions. 

I was particularly intrigued by the way saliency maps can be used to explain the importance
of groups of pixels in an image [1]. This way, mistakes that a model makes can be easily visualized:
if a group of pixels is deemed as important by the model, but it's a false positive, actions can 
be taken to improve the model based on that edge case.


\subsubsection{Sources}

[1] `Saliency map' Wikipedia, Oct. 2021. [Online]. 

\hspace{1.5em} Available: \url{https://en.wikipedia.org/w/index.php?title=Saliency\_map\&oldid=1049691575}

\newpage
\subsection{Installation manual}

\subsubsection{System requirements}

\begin{itemize}
	\item An Nvidia GPU with at least 3 GB of VRAM
	\item A good CPU that can handle the MCTS algorithm
	\item At least 5 GB of free RAM
\end{itemize}

\subsubsection{Python packages (for local/non-dockerized use)}

\begin{enumerate}
	\item Install the latest version of Python (I used Python 3.10). 
	\item In a terminal, run the following commands:
\end{enumerate}

\begin{minted}{sh}
python3 -m pip install pip --upgrade
python3 -m pip install -r requirements.client.text
python3 -m pip install -r requirements.server.text
\end{minted}

\subsubsection{Docker}

The software comes with two Docker images and a docker-compose file.
This is used to create a training set using self-play. 
To make sure the GPU can be used inside the server's Docker container,
follow these instructions to install Docker and the Docker utility engine:

\url{https://docs.nvidia.com/ai-enterprise/deployment-guide/dg-docker.html}

Using these images, it is possible to deploy a whole cluster of servers and clients,
to create a training set in parallel.


\newpage
\subsection{User manual}

\subsubsection{Introduction}

I made a chess engine that uses deep reinforcement learning to 
play moves. It is possible to host the AI model on a GPU-equipped server, or
host the model locally.

This software has multiple features:

\begin{itemize}
	\item Create a training set using a specific AI model, by playing chess games against itself.
	\item Deploy a whole cluster of servers and clients to create a training set in parallel.
	\item Train the network using a training set created by self-play.
	\item Evaluate two different AI models by playing a given number of games against each other.
	\item Play against an AI
\end{itemize}


To create a training set, you first need to either create your own AI model, or use my pretrained model.

\subsubsection{Create your own AI model}

To create your own AI model, run the following command:

\begin{minted}{shell}
python rlmodelbuilder.py --model-folder <FOLDER> --model-name <NAME> 

# For a detailed description of the parameters, run:
python rlmodelbuilder.py --help
\end{minted}

If you want to change parameters like the number of hidden layers, 
the input and output shapes (if you want to use the AI for other games), or the amount of convolution filters, 
change values in the config.py file.

\subsubsection{Use my pretrained model}

You can find a link to my pretrained model in the README.md file in the repository.
Copy the model.h5 file to the `models/' folder.

\subsubsection{Create a training set using the chosen model}

There are two ways to create a training set:

\begin{itemize}
	\item Play full chess games with one model playing white, and a copy of the model playing black.
	\item Solve chess puzzles using the AI model.
\end{itemize}

The first method can easily be deployed using the docker-compose file:

\begin{minted}{shell}
# in repository's code/ folder:
docker-compose up --build
\end{minted}

This will deploy one prediction server and 8 clients $\Rightarrow$ 8 parallel games will play. 
The data for the training set will be stored in ./memory.
The amount of clients can be changed in the docker-compose file. 
You can also use the two docker images in a cluster like Kubernetes, to
reliably deploy many more servers and clients in parallel.

Using the created GUI, it is possible to visualize the self-play of all these boards in real-time.
This can be done by settings the `SELFPLAY\_SHOW\_BOARD' environment variable in docker-compose.yml to `true'.
For every replica of the client, a new PyGame window will open where the board will change in real-time. 

To manually run self-play or to solve puzzles, you can run the selfplay.py file 
with specific arguments. For a detailed description of the arguments, run:

\begin{minted}{shell}
python3 selfplay.py --help
\end{minted}

\subsubsection{Manual (non-dockerized) self-play with full games}

\begin{minted}{shell}
# if you want to send predictions to a server, start the server first:
python3 server.py
\end{minted}


\begin{minted}{shell}
# if you want to predict locally instead of on a server, add --local-predictions:
python3 selfplay.py --local-predictions
\end{minted}

The games will be saved in the './memory' folder.

\subsubsection{Solving puzzles (non-dockerized)}

You can download the puzzles file here: \url{https://database.lichess.org/#puzzles}.
This is a .csv file consisting of more than 2 million chess puzzles, 
with many different types (`mateIn1', `mateIn2', `endgame', `short', etc\dots).

Training with these puzzles is faster than training with full games, and the AI can
more easily learn patterns like mating moves, or other patterns that are difficult to solve. 

\begin{minted}{shell}
python3 selfplay.py --type puzzles \
    --puzzle-file <PATH-TO-CSV-FILE> \
    --puzle-type <TYPE>
\end{minted}

Just like with self-play, you can add -{}-local-predictions if you want to predict locally instead of on a server.
If the puzzle ends in a checkmate within the move limit, the game will be saved to memory. That is why for now, only puzzles of type `mateInX' are supported.
The puzzle move limit can be changed in the config.py file.

\subsubsection{Training the AI using a created training set}

\begin{minted}{shell}
python3 train.py \
    --model <PATH-TO-MODEL> \
    --data-folder <PATH-TO-TRAINING-SET-FOLDER>

# For a full description of the parameters, run:
python3 train.py --help
\end{minted}

You can change parameters like batch size and learning rate in the config.py file. 

\subsubsection{Evaluating two models}

\begin{minted}{shell}
python3 evaluate.py evaluate.py <model_1> <model_2> <nr_games>

# For example, to run 100 matches between two models, run:
python3 evaluate.py models/model_1.h5 models/model_2.h5 100

# For a full description of the parameters, run:
python3 evaluate.py --help
\end{minted}

The white player in chess inherently has a slightly higher chance of winning,
because they can play the first move. Therefore, to evaluate two models, 
each model has to play as white and black an equal number of games.

Therefore, running $n$ matches means the evaluation will consist of $2\cdot n$ games, because
each model will play both colors once per match.

The output of this command will be an overview of the results of the matches:

\begin{minted}{text}
Evaluated these models for 100 matches: 
    Model 1 = models/model_1.h5, Model 2 = models/model_2.h5 
The results: 
Model 1: X wins
Model 2: X wins
Draws: X
\end{minted}

\subsubsection{Playing against the AI}

To play against the AI, you can run the following command:

\begin{minted}{shell}
# [brackets] indicate optional arguments
python3 main.py [--player <white|black>] \
    [--local-predictions] \
    [--model <PATH-TO-MODEL>]
\end{minted}


\begin{itemize}
    \item If you don't specify a player, a random side will be chosen for you.
    \item If you add -{}-local-predictions, you also have to specify a model.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{img/chessboard-gui.png}
    \caption{The chessboard GUI}
\end{figure}

\begin{itemize}
    \item You can click on a piece to move it, then click a destination square
    \item When you click a piece, the square lights up to show it is selected 
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{img/chessboard-promotion.png}
    \caption{Promoting a pawn}
\end{figure}

\subsubsection{Changing the AI difficulty}

\begin{itemize}
    \item You can change the difficulty of the AI by changing the amount of MCTS simulations per move in the config.py file.
    \item This will also change the amount of time it takes for the AI to make a move.
\end{itemize}

% TODO: meer bijlages



% nakaft
\afterpage{\blankpage}


\end{document}